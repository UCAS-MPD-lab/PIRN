{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e426beac-31b7-4304-9f2d-f87300c8c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Loss: 0.8894960880279541, PDE Loss: 0.00016058312030509114, BC Loss: 0.8893355131149292\n",
      "Epoch 10, Total Loss: 0.4934851825237274, PDE Loss: 6.827798415542929e-07, BC Loss: 0.4934844970703125\n",
      "Epoch 20, Total Loss: 0.23382219672203064, PDE Loss: 0.0016793678514659405, BC Loss: 0.232142835855484\n",
      "Epoch 30, Total Loss: 0.1624755561351776, PDE Loss: 0.017759229987859726, BC Loss: 0.1447163224220276\n",
      "Epoch 40, Total Loss: 0.15193217992782593, PDE Loss: 0.03733969107270241, BC Loss: 0.11459248512983322\n",
      "Epoch 50, Total Loss: 0.12618932127952576, PDE Loss: 0.03381095454096794, BC Loss: 0.09237836301326752\n",
      "Epoch 60, Total Loss: 0.11776483058929443, PDE Loss: 0.03379520773887634, BC Loss: 0.08396962285041809\n",
      "Epoch 70, Total Loss: 0.11165773868560791, PDE Loss: 0.04670729115605354, BC Loss: 0.06495044380426407\n",
      "Epoch 80, Total Loss: 0.11050085723400116, PDE Loss: 0.056190039962530136, BC Loss: 0.054310817271471024\n",
      "Epoch 90, Total Loss: 0.1102554202079773, PDE Loss: 0.05429616943001747, BC Loss: 0.055959247052669525\n",
      "Epoch 100, Total Loss: 0.11026515066623688, PDE Loss: 0.054496847093105316, BC Loss: 0.05576830357313156\n",
      "Epoch 110, Total Loss: 0.11017926037311554, PDE Loss: 0.055600326508283615, BC Loss: 0.05457893759012222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m u_boundary \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m1.0\u001b[39m]])\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_collocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_boundary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m, in \u001b[0;36mPINNTrainer.train\u001b[0;34m(self, collocation_points, boundary_points, boundary_values, epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 计算 collocation 点处的 PDE 残差\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollocation_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m pde_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(res, torch\u001b[38;5;241m.\u001b[39mzeros_like(res))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 计算边界条件损失\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mPINNTrainer.pde_residual\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     50\u001b[0m x\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 51\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 计算一阶导数 u_x\u001b[39;00m\n\u001b[1;32m     53\u001b[0m u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x,\n\u001b[1;32m     54\u001b[0m                           grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u),\n\u001b[1;32m     55\u001b[0m                           retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m                           create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36mPINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义 PINN 网络\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            layers: 一个列表，定义各层的神经元个数，例如 [1, 20, 20, 20, 1]\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        layer_list = []\n",
    "        for i in range(len(layers)-2):\n",
    "            layer_list.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            layer_list.append(self.activation)\n",
    "        # 最后一层不使用激活函数\n",
    "        layer_list.append(nn.Linear(layers[-2], layers[-1]))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 定义 PINN 训练器\n",
    "class PINNTrainer:\n",
    "    def __init__(self, model, nu, lr=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            model: PINN 模型\n",
    "            nu: 物理参数（粘性系数）\n",
    "            lr: 学习率\n",
    "            device: 训练设备，例如 'cpu' 或 'cuda'\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def pde_residual(self, x):\n",
    "        \"\"\"\n",
    "        计算 PDE 残差:\n",
    "            u * u_x - nu * u_xx\n",
    "        参数:\n",
    "            x: 输入的 collocation 点 (需要计算梯度)\n",
    "        \"\"\"\n",
    "        # 将 x 设置到对应设备并开启梯度\n",
    "        x = x.to(self.device)\n",
    "        x.requires_grad_(True)\n",
    "        u = self.model(x)\n",
    "        # 计算一阶导数 u_x\n",
    "        u_x = torch.autograd.grad(u, x,\n",
    "                                  grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True,\n",
    "                                  create_graph=True)[0]\n",
    "        # 计算二阶导数 u_xx\n",
    "        u_xx = torch.autograd.grad(u_x, x,\n",
    "                                   grad_outputs=torch.ones_like(u_x),\n",
    "                                   retain_graph=True,\n",
    "                                   create_graph=True)[0]\n",
    "        # 一维稳态 Burgers 方程: u * u_x - nu * u_xx = 0\n",
    "        residual = u * u_x - self.nu * u_xx\n",
    "        return residual\n",
    "\n",
    "    def train(self, collocation_points, boundary_points, boundary_values, epochs=10000):\n",
    "        \"\"\"\n",
    "        训练函数\n",
    "        参数:\n",
    "            collocation_points: 用于计算 PDE 残差的点（内部点）\n",
    "            boundary_points: 边界点\n",
    "            boundary_values: 对应的边界条件值\n",
    "            epochs: 训练的迭代次数\n",
    "        \"\"\"\n",
    "        collocation_points = collocation_points.to(self.device)\n",
    "        boundary_points = boundary_points.to(self.device)\n",
    "        boundary_values = boundary_values.to(self.device)\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            # 计算 collocation 点处的 PDE 残差\n",
    "            res = self.pde_residual(collocation_points)\n",
    "            pde_loss = self.loss_func(res, torch.zeros_like(res))\n",
    "            # 计算边界条件损失\n",
    "            u_b = self.model(boundary_points)\n",
    "            bc_loss = self.loss_func(u_b, boundary_values)\n",
    "            # 总损失\n",
    "            loss = pde_loss + bc_loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Total Loss: {loss.item()}, PDE Loss: {pde_loss.item()}, BC Loss: {bc_loss.item()}\")\n",
    "\n",
    "# 示例：训练求解一维稳态 Burgers 方程\n",
    "if __name__ == \"__main__\":\n",
    "    # 物理参数：粘性系数 (例如 nu = 0.01/π)\n",
    "    nu = 0.01 / torch.pi\n",
    "    # 定义神经网络结构：输入层1个神经元，三个隐藏层（每层20个神经元），输出层1个神经元\n",
    "    layers = [1, 20, 20, 20, 1]\n",
    "    model = PINN(layers)\n",
    "    trainer = PINNTrainer(model, nu, lr=1e-3, device='cpu')\n",
    "\n",
    "    # 构造 collocation 点（用于计算 PDE 残差）\n",
    "    N_collocation = 1000\n",
    "    # 在区间 [0, 1] 上均匀采样\n",
    "    x_collocation = torch.linspace(0, 1, N_collocation).view(-1, 1)\n",
    "\n",
    "    # 定义边界点和边界条件（这里假设 u(0)=0, u(1)=1，可根据问题实际设置）\n",
    "    x_boundary = torch.tensor([[0.0], [1.0]])\n",
    "    u_boundary = torch.tensor([[0.0], [1.0]])\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train(x_collocation, x_boundary, u_boundary, epochs=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad12edb-e4fb-4cb0-9740-5854ceff4814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
