{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0275341-ad74-4af1-bcd5-b189db22d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Networks.iResNet import iResNet3D\n",
    "from Saving.saveFile import get_next_model_filename\n",
    "from Trainers import split_data,Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79406ac-697c-4519-9c87-040dca2feb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "input_shape = (2,)  # 输入形状\n",
    "full_connect_shape = (128, 32, 32, 16)  # 全连接层目标形状，根据 true_output0 的形状确定\n",
    "q = 2  # BB 的数量\n",
    "N = 3  # 每个 BB 中 SB 的数量\n",
    "\n",
    "output_keys = [\"T\", \"w\"]\n",
    "\n",
    "model = iResNet3D(input_shape, full_connect_shape, q, N, output_keys)\n",
    "\n",
    "# # 打印模型结构\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd2c0ba-e524-4e31-b4a9-4c22e54776e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_np = joblib.load('preData/nonuni/input_all.joblib').astype(np.float32)\n",
    "T_train_np = joblib.load('preData/nonuni/T_train.joblib').astype(np.float32)\n",
    "w_train_np = joblib.load('preData/nonuni/w_train.joblib').astype(np.float32)\n",
    "\n",
    "input_train = torch.from_numpy(input_np).unsqueeze(1)\n",
    "T_train = torch.from_numpy(T_train_np).unsqueeze(1)\n",
    "w_train = torch.from_numpy(w_train_np).unsqueeze(1)\n",
    "\n",
    "targets_train = {\"T\": T_train, \"w\": w_train}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cb7162-ac4d-453d-bc1e-fed12a2bd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterions = {\n",
    "    \"T\": nn.MSELoss(),  # 针对 T 的损失函数\n",
    "    \"w\": nn.MSELoss(),  # 针对 w 的损失函数\n",
    "}\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10ba6fa-27e7-469e-94e0-91e593440663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "train_loader, val_loader = split_data(input_train, targets_train, batch_size=1000)\n",
    "trainer = Trainer(model, optimizer, criterions, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f553f496-93d5-4959-82c3-f12287219eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a4d6503dc147dabd262f68684616c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000 | Train Loss: 88.4031 | Val Loss: 7619.4600\n",
      "Epoch 2/100000 | Train Loss: 7665.2964 | Val Loss: 59.6387\n",
      "Epoch 3/100000 | Train Loss: 49.6805 | Val Loss: 95.2803\n",
      "Epoch 4/100000 | Train Loss: 75.7390 | Val Loss: 77.1364\n",
      "Epoch 5/100000 | Train Loss: 61.9260 | Val Loss: 72.9369\n",
      "Epoch 6/100000 | Train Loss: 72.9017 | Val Loss: 76.9992\n",
      "Epoch 7/100000 | Train Loss: 80.4417 | Val Loss: 63.1741\n",
      "Epoch 8/100000 | Train Loss: 61.6229 | Val Loss: 58.5213\n",
      "Epoch 9/100000 | Train Loss: 50.2110 | Val Loss: 65.9262\n",
      "Epoch 10/100000 | Train Loss: 52.8563 | Val Loss: 72.6172\n",
      "Epoch 11/100000 | Train Loss: 57.3093 | Val Loss: 74.9598\n",
      "Epoch 12/100000 | Train Loss: 58.9676 | Val Loss: 74.0924\n",
      "Epoch 13/100000 | Train Loss: 58.2769 | Val Loss: 70.7814\n",
      "Epoch 14/100000 | Train Loss: 55.7708 | Val Loss: 65.5254\n",
      "Epoch 15/100000 | Train Loss: 51.8936 | Val Loss: 58.8316\n",
      "Epoch 16/100000 | Train Loss: 47.1766 | Val Loss: 51.5676\n",
      "Epoch 17/100000 | Train Loss: 42.5130 | Val Loss: 45.3132\n",
      "Epoch 18/100000 | Train Loss: 39.3686 | Val Loss: 42.0520\n",
      "Epoch 19/100000 | Train Loss: 39.2528 | Val Loss: 41.9037\n",
      "Epoch 20/100000 | Train Loss: 41.2989 | Val Loss: 41.5552\n",
      "Epoch 21/100000 | Train Loss: 41.3136 | Val Loss: 39.4375\n",
      "Epoch 22/100000 | Train Loss: 37.9117 | Val Loss: 37.9754\n",
      "Epoch 23/100000 | Train Loss: 34.4501 | Val Loss: 38.3540\n",
      "Epoch 24/100000 | Train Loss: 33.0084 | Val Loss: 39.3516\n",
      "Epoch 25/100000 | Train Loss: 32.8135 | Val Loss: 39.8761\n",
      "Epoch 26/100000 | Train Loss: 32.7668 | Val Loss: 39.4044\n",
      "Epoch 27/100000 | Train Loss: 32.2959 | Val Loss: 37.8953\n",
      "Epoch 28/100000 | Train Loss: 31.2799 | Val Loss: 35.5545\n",
      "Epoch 29/100000 | Train Loss: 29.8481 | Val Loss: 32.7658\n",
      "Epoch 30/100000 | Train Loss: 28.2909 | Val Loss: 30.0589\n",
      "Epoch 31/100000 | Train Loss: 26.9921 | Val Loss: 27.9646\n",
      "Epoch 32/100000 | Train Loss: 26.2665 | Val Loss: 26.7427\n",
      "Epoch 33/100000 | Train Loss: 26.1037 | Val Loss: 26.1564\n",
      "Epoch 34/100000 | Train Loss: 26.0423 | Val Loss: 25.7138\n",
      "Epoch 35/100000 | Train Loss: 25.5283 | Val Loss: 25.2161\n",
      "Epoch 36/100000 | Train Loss: 24.4979 | Val Loss: 24.8916\n",
      "Epoch 37/100000 | Train Loss: 23.4163 | Val Loss: 24.9253\n",
      "Epoch 38/100000 | Train Loss: 22.6939 | Val Loss: 25.1664\n",
      "Epoch 39/100000 | Train Loss: 22.3396 | Val Loss: 25.2939\n",
      "Epoch 40/100000 | Train Loss: 22.1313 | Val Loss: 25.0481\n",
      "Epoch 41/100000 | Train Loss: 21.8447 | Val Loss: 24.3242\n",
      "Epoch 42/100000 | Train Loss: 21.3631 | Val Loss: 23.2126\n",
      "Epoch 43/100000 | Train Loss: 20.7189 | Val Loss: 21.9660\n",
      "Epoch 44/100000 | Train Loss: 20.0691 | Val Loss: 20.8912\n",
      "Epoch 45/100000 | Train Loss: 19.5936 | Val Loss: 20.1686\n",
      "Epoch 46/100000 | Train Loss: 19.3427 | Val Loss: 19.7343\n",
      "Epoch 47/100000 | Train Loss: 19.1590 | Val Loss: 19.3964\n",
      "Epoch 48/100000 | Train Loss: 18.8277 | Val Loss: 19.0855\n",
      "Epoch 49/100000 | Train Loss: 18.3206 | Val Loss: 18.8919\n",
      "Epoch 50/100000 | Train Loss: 17.8152 | Val Loss: 18.8568\n",
      "Epoch 51/100000 | Train Loss: 17.4544 | Val Loss: 18.8584\n",
      "Epoch 52/100000 | Train Loss: 17.2095 | Val Loss: 18.7270\n",
      "Epoch 53/100000 | Train Loss: 16.9686 | Val Loss: 18.3639\n",
      "Epoch 54/100000 | Train Loss: 16.6501 | Val Loss: 17.7957\n",
      "Epoch 55/100000 | Train Loss: 16.2597 | Val Loss: 17.1543\n",
      "Epoch 56/100000 | Train Loss: 15.8772 | Val Loss: 16.5817\n",
      "Epoch 57/100000 | Train Loss: 15.5763 | Val Loss: 16.1319\n",
      "Epoch 58/100000 | Train Loss: 15.3482 | Val Loss: 15.7666\n",
      "Epoch 59/100000 | Train Loss: 15.1138 | Val Loss: 15.4457\n",
      "Epoch 60/100000 | Train Loss: 14.8200 | Val Loss: 15.1928\n",
      "Epoch 61/100000 | Train Loss: 14.4986 | Val Loss: 15.0372\n",
      "Epoch 62/100000 | Train Loss: 14.2125 | Val Loss: 14.9446\n",
      "Epoch 63/100000 | Train Loss: 13.9760 | Val Loss: 14.8310\n",
      "Epoch 64/100000 | Train Loss: 13.7547 | Val Loss: 14.6276\n",
      "Epoch 65/100000 | Train Loss: 13.5140 | Val Loss: 14.3267\n",
      "Epoch 66/100000 | Train Loss: 13.2548 | Val Loss: 13.9685\n",
      "Epoch 67/100000 | Train Loss: 13.0014 | Val Loss: 13.5991\n",
      "Epoch 68/100000 | Train Loss: 12.7708 | Val Loss: 13.2496\n",
      "Epoch 69/100000 | Train Loss: 12.5572 | Val Loss: 12.9362\n",
      "Epoch 70/100000 | Train Loss: 12.3440 | Val Loss: 12.6761\n",
      "Epoch 71/100000 | Train Loss: 12.1246 | Val Loss: 12.4784\n",
      "Epoch 72/100000 | Train Loss: 11.9056 | Val Loss: 12.3336\n",
      "Epoch 73/100000 | Train Loss: 11.6940 | Val Loss: 12.2127\n",
      "Epoch 74/100000 | Train Loss: 11.4919 | Val Loss: 12.0810\n",
      "Epoch 75/100000 | Train Loss: 11.2976 | Val Loss: 11.9110\n",
      "Epoch 76/100000 | Train Loss: 11.1069 | Val Loss: 11.6895\n",
      "Epoch 77/100000 | Train Loss: 10.9149 | Val Loss: 11.4288\n",
      "Epoch 78/100000 | Train Loss: 10.7232 | Val Loss: 11.1617\n",
      "Epoch 79/100000 | Train Loss: 10.5387 | Val Loss: 10.9221\n",
      "Epoch 80/100000 | Train Loss: 10.3655 | Val Loss: 10.7247\n",
      "Epoch 81/100000 | Train Loss: 10.1968 | Val Loss: 10.5665\n",
      "Epoch 82/100000 | Train Loss: 10.0246 | Val Loss: 10.4390\n",
      "Epoch 83/100000 | Train Loss: 9.8522 | Val Loss: 10.3296\n",
      "Epoch 84/100000 | Train Loss: 9.6886 | Val Loss: 10.2154\n",
      "Epoch 85/100000 | Train Loss: 9.5347 | Val Loss: 10.0714\n",
      "Epoch 86/100000 | Train Loss: 9.3820 | Val Loss: 9.8928\n",
      "Epoch 87/100000 | Train Loss: 9.2259 | Val Loss: 9.7024\n",
      "Epoch 88/100000 | Train Loss: 9.0738 | Val Loss: 9.5275\n",
      "Epoch 89/100000 | Train Loss: 8.9310 | Val Loss: 9.3792\n",
      "Epoch 90/100000 | Train Loss: 8.7927 | Val Loss: 9.2538\n",
      "Epoch 91/100000 | Train Loss: 8.6526 | Val Loss: 9.1472\n",
      "Epoch 92/100000 | Train Loss: 8.5139 | Val Loss: 9.0499\n",
      "Epoch 93/100000 | Train Loss: 8.3819 | Val Loss: 8.9462\n",
      "Epoch 94/100000 | Train Loss: 8.2547 | Val Loss: 8.8257\n",
      "Epoch 95/100000 | Train Loss: 8.1285 | Val Loss: 8.6942\n",
      "Epoch 96/100000 | Train Loss: 8.0033 | Val Loss: 8.5673\n",
      "Epoch 97/100000 | Train Loss: 7.8827 | Val Loss: 8.4547\n",
      "Epoch 98/100000 | Train Loss: 7.7662 | Val Loss: 8.3562\n",
      "Epoch 99/100000 | Train Loss: 7.6516 | Val Loss: 8.2671\n",
      "Epoch 100/100000 | Train Loss: 7.5388 | Val Loss: 8.1806\n",
      "Epoch 101/100000 | Train Loss: 7.4292 | Val Loss: 8.0900\n",
      "Epoch 102/100000 | Train Loss: 7.3227 | Val Loss: 7.9946\n",
      "Epoch 103/100000 | Train Loss: 7.2185 | Val Loss: 7.8995\n",
      "Epoch 104/100000 | Train Loss: 7.1167 | Val Loss: 7.8109\n",
      "Epoch 105/100000 | Train Loss: 7.0170 | Val Loss: 7.7317\n",
      "Epoch 106/100000 | Train Loss: 6.9197 | Val Loss: 7.6598\n",
      "Epoch 107/100000 | Train Loss: 6.8250 | Val Loss: 7.5893\n",
      "Epoch 108/100000 | Train Loss: 6.7329 | Val Loss: 7.5139\n",
      "Epoch 109/100000 | Train Loss: 6.6421 | Val Loss: 7.4346\n",
      "Epoch 110/100000 | Train Loss: 6.5533 | Val Loss: 7.3569\n",
      "Epoch 111/100000 | Train Loss: 6.4672 | Val Loss: 7.2853\n",
      "Epoch 112/100000 | Train Loss: 6.3830 | Val Loss: 7.2217\n",
      "Epoch 113/100000 | Train Loss: 6.3003 | Val Loss: 7.1639\n",
      "Epoch 114/100000 | Train Loss: 6.2193 | Val Loss: 7.1075\n",
      "Epoch 115/100000 | Train Loss: 6.1409 | Val Loss: 7.0458\n",
      "Epoch 116/100000 | Train Loss: 6.0635 | Val Loss: 6.9790\n",
      "Epoch 117/100000 | Train Loss: 5.9877 | Val Loss: 6.9121\n",
      "Epoch 118/100000 | Train Loss: 5.9140 | Val Loss: 6.8499\n",
      "Epoch 119/100000 | Train Loss: 5.8419 | Val Loss: 6.7942\n",
      "Epoch 120/100000 | Train Loss: 5.7708 | Val Loss: 6.7439\n",
      "Epoch 121/100000 | Train Loss: 5.7011 | Val Loss: 6.6947\n",
      "Epoch 122/100000 | Train Loss: 5.6330 | Val Loss: 6.6422\n",
      "Epoch 123/100000 | Train Loss: 5.5663 | Val Loss: 6.5846\n",
      "Epoch 124/100000 | Train Loss: 5.5006 | Val Loss: 6.5258\n",
      "Epoch 125/100000 | Train Loss: 5.4363 | Val Loss: 6.4696\n",
      "Epoch 126/100000 | Train Loss: 5.3733 | Val Loss: 6.4182\n",
      "Epoch 127/100000 | Train Loss: 5.3113 | Val Loss: 6.3714\n",
      "Epoch 128/100000 | Train Loss: 5.2505 | Val Loss: 6.3256\n",
      "Epoch 129/100000 | Train Loss: 5.1909 | Val Loss: 6.2775\n",
      "Epoch 130/100000 | Train Loss: 5.1323 | Val Loss: 6.2264\n",
      "Epoch 131/100000 | Train Loss: 5.0749 | Val Loss: 6.1736\n",
      "Epoch 132/100000 | Train Loss: 5.0185 | Val Loss: 6.1220\n",
      "Epoch 133/100000 | Train Loss: 4.9628 | Val Loss: 6.0734\n",
      "Epoch 134/100000 | Train Loss: 4.9082 | Val Loss: 6.0280\n",
      "Epoch 135/100000 | Train Loss: 4.8544 | Val Loss: 5.9836\n",
      "Epoch 136/100000 | Train Loss: 4.8017 | Val Loss: 5.9380\n",
      "Epoch 137/100000 | Train Loss: 4.7497 | Val Loss: 5.8899\n",
      "Epoch 138/100000 | Train Loss: 4.6984 | Val Loss: 5.8400\n",
      "Epoch 139/100000 | Train Loss: 4.6480 | Val Loss: 5.7909\n",
      "Epoch 140/100000 | Train Loss: 4.5984 | Val Loss: 5.7455\n",
      "Epoch 141/100000 | Train Loss: 4.5497 | Val Loss: 5.7025\n",
      "Epoch 142/100000 | Train Loss: 4.5018 | Val Loss: 5.6607\n",
      "Epoch 143/100000 | Train Loss: 4.4547 | Val Loss: 5.6174\n",
      "Epoch 144/100000 | Train Loss: 4.4083 | Val Loss: 5.5719\n",
      "Epoch 145/100000 | Train Loss: 4.3627 | Val Loss: 5.5251\n",
      "Epoch 146/100000 | Train Loss: 4.3176 | Val Loss: 5.4796\n",
      "Epoch 147/100000 | Train Loss: 4.2734 | Val Loss: 5.4371\n",
      "Epoch 148/100000 | Train Loss: 4.2297 | Val Loss: 5.3962\n",
      "Epoch 149/100000 | Train Loss: 4.1866 | Val Loss: 5.3555\n",
      "Epoch 150/100000 | Train Loss: 4.1442 | Val Loss: 5.3120\n",
      "Epoch 151/100000 | Train Loss: 4.1023 | Val Loss: 5.2664\n",
      "Epoch 152/100000 | Train Loss: 4.0609 | Val Loss: 5.2219\n",
      "Epoch 153/100000 | Train Loss: 4.0203 | Val Loss: 5.1810\n",
      "Epoch 154/100000 | Train Loss: 3.9803 | Val Loss: 5.1416\n",
      "Epoch 155/100000 | Train Loss: 3.9407 | Val Loss: 5.1031\n",
      "Epoch 156/100000 | Train Loss: 3.9019 | Val Loss: 5.0630\n",
      "Epoch 157/100000 | Train Loss: 3.8635 | Val Loss: 5.0212\n",
      "Epoch 158/100000 | Train Loss: 3.8258 | Val Loss: 4.9813\n",
      "Epoch 159/100000 | Train Loss: 3.7886 | Val Loss: 4.9427\n",
      "Epoch 160/100000 | Train Loss: 3.7518 | Val Loss: 4.9060\n",
      "Epoch 161/100000 | Train Loss: 3.7155 | Val Loss: 4.8702\n",
      "Epoch 162/100000 | Train Loss: 3.6798 | Val Loss: 4.8338\n",
      "Epoch 163/100000 | Train Loss: 3.6447 | Val Loss: 4.7964\n",
      "Epoch 164/100000 | Train Loss: 3.6101 | Val Loss: 4.7595\n",
      "Epoch 165/100000 | Train Loss: 3.5758 | Val Loss: 4.7241\n",
      "Epoch 166/100000 | Train Loss: 3.5421 | Val Loss: 4.6900\n",
      "Epoch 167/100000 | Train Loss: 3.5088 | Val Loss: 4.6562\n",
      "Epoch 168/100000 | Train Loss: 3.4760 | Val Loss: 4.6222\n",
      "Epoch 169/100000 | Train Loss: 3.4435 | Val Loss: 4.5879\n",
      "Epoch 170/100000 | Train Loss: 3.4116 | Val Loss: 4.5541\n",
      "Epoch 171/100000 | Train Loss: 3.3801 | Val Loss: 4.5214\n",
      "Epoch 172/100000 | Train Loss: 3.3490 | Val Loss: 4.4895\n",
      "Epoch 173/100000 | Train Loss: 3.3183 | Val Loss: 4.4582\n",
      "Epoch 174/100000 | Train Loss: 3.2881 | Val Loss: 4.4275\n",
      "Epoch 175/100000 | Train Loss: 3.2581 | Val Loss: 4.3969\n",
      "Epoch 176/100000 | Train Loss: 3.2286 | Val Loss: 4.3663\n",
      "Epoch 177/100000 | Train Loss: 3.1995 | Val Loss: 4.3368\n",
      "Epoch 178/100000 | Train Loss: 3.1707 | Val Loss: 4.3082\n",
      "Epoch 179/100000 | Train Loss: 3.1424 | Val Loss: 4.2798\n",
      "Epoch 180/100000 | Train Loss: 3.1143 | Val Loss: 4.2524\n",
      "Epoch 181/100000 | Train Loss: 3.0867 | Val Loss: 4.2249\n",
      "Epoch 182/100000 | Train Loss: 3.0594 | Val Loss: 4.1971\n",
      "Epoch 183/100000 | Train Loss: 3.0324 | Val Loss: 4.1699\n",
      "Epoch 184/100000 | Train Loss: 3.0058 | Val Loss: 4.1435\n",
      "Epoch 185/100000 | Train Loss: 2.9794 | Val Loss: 4.1173\n",
      "Epoch 186/100000 | Train Loss: 2.9534 | Val Loss: 4.0913\n",
      "Epoch 187/100000 | Train Loss: 2.9277 | Val Loss: 4.0655\n",
      "Epoch 188/100000 | Train Loss: 2.9023 | Val Loss: 4.0402\n",
      "Epoch 189/100000 | Train Loss: 2.8773 | Val Loss: 4.0158\n",
      "Epoch 190/100000 | Train Loss: 2.8524 | Val Loss: 3.9920\n",
      "Epoch 191/100000 | Train Loss: 2.8279 | Val Loss: 3.9677\n",
      "Epoch 192/100000 | Train Loss: 2.8035 | Val Loss: 3.9444\n",
      "Epoch 193/100000 | Train Loss: 2.7795 | Val Loss: 3.9217\n",
      "Epoch 194/100000 | Train Loss: 2.7557 | Val Loss: 3.8992\n",
      "Epoch 195/100000 | Train Loss: 2.7323 | Val Loss: 3.8764\n",
      "Epoch 196/100000 | Train Loss: 2.7090 | Val Loss: 3.8537\n",
      "Epoch 197/100000 | Train Loss: 2.6859 | Val Loss: 3.8318\n",
      "Epoch 198/100000 | Train Loss: 2.6630 | Val Loss: 3.8106\n",
      "Epoch 199/100000 | Train Loss: 2.6403 | Val Loss: 3.7891\n",
      "Epoch 200/100000 | Train Loss: 2.6179 | Val Loss: 3.7687\n",
      "Epoch 201/100000 | Train Loss: 2.5954 | Val Loss: 3.7483\n",
      "Epoch 202/100000 | Train Loss: 2.5732 | Val Loss: 3.7290\n",
      "Epoch 203/100000 | Train Loss: 2.5510 | Val Loss: 3.7101\n",
      "Epoch 204/100000 | Train Loss: 2.5289 | Val Loss: 3.6919\n",
      "Epoch 205/100000 | Train Loss: 2.5070 | Val Loss: 3.6734\n",
      "Epoch 206/100000 | Train Loss: 2.4852 | Val Loss: 3.6552\n",
      "Epoch 207/100000 | Train Loss: 2.4636 | Val Loss: 3.6382\n",
      "Epoch 208/100000 | Train Loss: 2.4422 | Val Loss: 3.6224\n",
      "Epoch 209/100000 | Train Loss: 2.4213 | Val Loss: 3.6061\n",
      "Epoch 210/100000 | Train Loss: 2.4008 | Val Loss: 3.5891\n",
      "Epoch 211/100000 | Train Loss: 2.3807 | Val Loss: 3.5717\n",
      "Epoch 212/100000 | Train Loss: 2.3610 | Val Loss: 3.5542\n",
      "Epoch 213/100000 | Train Loss: 2.3415 | Val Loss: 3.5334\n",
      "Epoch 214/100000 | Train Loss: 2.3221 | Val Loss: 3.5097\n",
      "Epoch 215/100000 | Train Loss: 2.3030 | Val Loss: 3.4850\n",
      "Epoch 216/100000 | Train Loss: 2.2841 | Val Loss: 3.4614\n",
      "Epoch 217/100000 | Train Loss: 2.2655 | Val Loss: 3.4374\n",
      "Epoch 218/100000 | Train Loss: 2.2472 | Val Loss: 3.4125\n",
      "Epoch 219/100000 | Train Loss: 2.2292 | Val Loss: 3.3885\n",
      "Epoch 220/100000 | Train Loss: 2.2113 | Val Loss: 3.3656\n",
      "Epoch 221/100000 | Train Loss: 2.1936 | Val Loss: 3.3434\n",
      "Epoch 222/100000 | Train Loss: 2.1759 | Val Loss: 3.3213\n",
      "Epoch 223/100000 | Train Loss: 2.1583 | Val Loss: 3.2992\n",
      "Epoch 224/100000 | Train Loss: 2.1411 | Val Loss: 3.2783\n",
      "Epoch 225/100000 | Train Loss: 2.1240 | Val Loss: 3.2583\n",
      "Epoch 226/100000 | Train Loss: 2.1073 | Val Loss: 3.2385\n",
      "Epoch 227/100000 | Train Loss: 2.0908 | Val Loss: 3.2165\n",
      "Epoch 228/100000 | Train Loss: 2.0743 | Val Loss: 3.1927\n",
      "Epoch 229/100000 | Train Loss: 2.0581 | Val Loss: 3.1700\n",
      "Epoch 230/100000 | Train Loss: 2.0419 | Val Loss: 3.1484\n",
      "Epoch 231/100000 | Train Loss: 2.0259 | Val Loss: 3.1262\n",
      "Epoch 232/100000 | Train Loss: 2.0101 | Val Loss: 3.1053\n",
      "Epoch 233/100000 | Train Loss: 1.9946 | Val Loss: 3.0854\n",
      "Epoch 234/100000 | Train Loss: 1.9793 | Val Loss: 3.0678\n",
      "Epoch 235/100000 | Train Loss: 1.9640 | Val Loss: 3.0492\n",
      "Epoch 236/100000 | Train Loss: 1.9489 | Val Loss: 3.0312\n",
      "Epoch 237/100000 | Train Loss: 1.9340 | Val Loss: 3.0136\n",
      "Epoch 238/100000 | Train Loss: 1.9194 | Val Loss: 2.9969\n",
      "Epoch 239/100000 | Train Loss: 1.9048 | Val Loss: 2.9802\n",
      "Epoch 240/100000 | Train Loss: 1.8905 | Val Loss: 2.9615\n",
      "Epoch 241/100000 | Train Loss: 1.8762 | Val Loss: 2.9428\n",
      "Epoch 242/100000 | Train Loss: 1.8622 | Val Loss: 2.9240\n",
      "Epoch 243/100000 | Train Loss: 1.8482 | Val Loss: 2.9058\n",
      "Epoch 244/100000 | Train Loss: 1.8345 | Val Loss: 2.8875\n",
      "Epoch 245/100000 | Train Loss: 1.8209 | Val Loss: 2.8706\n",
      "Epoch 246/100000 | Train Loss: 1.8075 | Val Loss: 2.8546\n",
      "Epoch 247/100000 | Train Loss: 1.7942 | Val Loss: 2.8383\n",
      "Epoch 248/100000 | Train Loss: 1.7810 | Val Loss: 2.8215\n",
      "Epoch 249/100000 | Train Loss: 1.7680 | Val Loss: 2.8060\n",
      "Epoch 250/100000 | Train Loss: 1.7552 | Val Loss: 2.7900\n",
      "Epoch 251/100000 | Train Loss: 1.7425 | Val Loss: 2.7731\n",
      "Epoch 252/100000 | Train Loss: 1.7300 | Val Loss: 2.7569\n",
      "Epoch 253/100000 | Train Loss: 1.7176 | Val Loss: 2.7402\n",
      "Epoch 254/100000 | Train Loss: 1.7054 | Val Loss: 2.7239\n",
      "Epoch 255/100000 | Train Loss: 1.6932 | Val Loss: 2.7080\n",
      "Epoch 256/100000 | Train Loss: 1.6813 | Val Loss: 2.6934\n",
      "Epoch 257/100000 | Train Loss: 1.6694 | Val Loss: 2.6779\n",
      "Epoch 258/100000 | Train Loss: 1.6578 | Val Loss: 2.6634\n",
      "Epoch 259/100000 | Train Loss: 1.6463 | Val Loss: 2.6491\n",
      "Epoch 260/100000 | Train Loss: 1.6349 | Val Loss: 2.6334\n",
      "Epoch 261/100000 | Train Loss: 1.6236 | Val Loss: 2.6188\n",
      "Epoch 262/100000 | Train Loss: 1.6124 | Val Loss: 2.6033\n",
      "Epoch 263/100000 | Train Loss: 1.6014 | Val Loss: 2.5879\n",
      "Epoch 264/100000 | Train Loss: 1.5905 | Val Loss: 2.5735\n",
      "Epoch 265/100000 | Train Loss: 1.5797 | Val Loss: 2.5596\n",
      "Epoch 266/100000 | Train Loss: 1.5691 | Val Loss: 2.5454\n",
      "Epoch 267/100000 | Train Loss: 1.5586 | Val Loss: 2.5318\n",
      "Epoch 268/100000 | Train Loss: 1.5482 | Val Loss: 2.5185\n",
      "Epoch 269/100000 | Train Loss: 1.5379 | Val Loss: 2.5047\n",
      "Epoch 270/100000 | Train Loss: 1.5278 | Val Loss: 2.4909\n",
      "Epoch 271/100000 | Train Loss: 1.5178 | Val Loss: 2.4780\n",
      "Epoch 272/100000 | Train Loss: 1.5078 | Val Loss: 2.4646\n",
      "Epoch 273/100000 | Train Loss: 1.4981 | Val Loss: 2.4509\n",
      "Epoch 274/100000 | Train Loss: 1.4884 | Val Loss: 2.4381\n",
      "Epoch 275/100000 | Train Loss: 1.4788 | Val Loss: 2.4251\n",
      "Epoch 276/100000 | Train Loss: 1.4694 | Val Loss: 2.4134\n",
      "Epoch 277/100000 | Train Loss: 1.4600 | Val Loss: 2.4014\n",
      "Epoch 278/100000 | Train Loss: 1.4508 | Val Loss: 2.3887\n",
      "Epoch 279/100000 | Train Loss: 1.4417 | Val Loss: 2.3760\n",
      "Epoch 280/100000 | Train Loss: 1.4327 | Val Loss: 2.3638\n",
      "Epoch 281/100000 | Train Loss: 1.4237 | Val Loss: 2.3518\n",
      "Epoch 282/100000 | Train Loss: 1.4149 | Val Loss: 2.3406\n",
      "Epoch 283/100000 | Train Loss: 1.4062 | Val Loss: 2.3285\n",
      "Epoch 284/100000 | Train Loss: 1.3976 | Val Loss: 2.3170\n",
      "Epoch 285/100000 | Train Loss: 1.3891 | Val Loss: 2.3056\n",
      "Epoch 286/100000 | Train Loss: 1.3807 | Val Loss: 2.2941\n",
      "Epoch 287/100000 | Train Loss: 1.3723 | Val Loss: 2.2843\n",
      "Epoch 288/100000 | Train Loss: 1.3641 | Val Loss: 2.2724\n",
      "Epoch 289/100000 | Train Loss: 1.3560 | Val Loss: 2.2615\n",
      "Epoch 290/100000 | Train Loss: 1.3480 | Val Loss: 2.2505\n",
      "Epoch 291/100000 | Train Loss: 1.3400 | Val Loss: 2.2398\n",
      "Epoch 292/100000 | Train Loss: 1.3321 | Val Loss: 2.2301\n",
      "Epoch 293/100000 | Train Loss: 1.3244 | Val Loss: 2.2187\n",
      "Epoch 294/100000 | Train Loss: 1.3167 | Val Loss: 2.2097\n",
      "Epoch 295/100000 | Train Loss: 1.3091 | Val Loss: 2.1979\n",
      "Epoch 296/100000 | Train Loss: 1.3016 | Val Loss: 2.1888\n",
      "Epoch 297/100000 | Train Loss: 1.2942 | Val Loss: 2.1784\n",
      "Epoch 298/100000 | Train Loss: 1.2868 | Val Loss: 2.1692\n",
      "Epoch 299/100000 | Train Loss: 1.2796 | Val Loss: 2.1589\n",
      "Epoch 300/100000 | Train Loss: 1.2724 | Val Loss: 2.1498\n",
      "Epoch 301/100000 | Train Loss: 1.2653 | Val Loss: 2.1397\n",
      "Epoch 302/100000 | Train Loss: 1.2583 | Val Loss: 2.1306\n",
      "Epoch 303/100000 | Train Loss: 1.2513 | Val Loss: 2.1210\n",
      "Epoch 304/100000 | Train Loss: 1.2444 | Val Loss: 2.1132\n",
      "Epoch 305/100000 | Train Loss: 1.2377 | Val Loss: 2.1010\n",
      "Epoch 306/100000 | Train Loss: 1.2310 | Val Loss: 2.0966\n",
      "Epoch 307/100000 | Train Loss: 1.2246 | Val Loss: 2.0811\n",
      "Epoch 308/100000 | Train Loss: 1.2186 | Val Loss: 2.0856\n",
      "Epoch 309/100000 | Train Loss: 1.2141 | Val Loss: 2.0606\n",
      "Epoch 310/100000 | Train Loss: 1.2130 | Val Loss: 2.1037\n",
      "Epoch 311/100000 | Train Loss: 1.2297 | Val Loss: 2.0504\n",
      "Epoch 312/100000 | Train Loss: 1.2385 | Val Loss: 2.1547\n",
      "Epoch 313/100000 | Train Loss: 1.3081 | Val Loss: 1.9947\n",
      "Epoch 314/100000 | Train Loss: 1.2017 | Val Loss: 2.0288\n",
      "Epoch 315/100000 | Train Loss: 1.2547 | Val Loss: 2.4275\n",
      "Epoch 316/100000 | Train Loss: 1.5077 | Val Loss: 1.9973\n",
      "Epoch 317/100000 | Train Loss: 1.2199 | Val Loss: 2.2105\n",
      "Epoch 318/100000 | Train Loss: 1.5297 | Val Loss: 2.4217\n",
      "Epoch 319/100000 | Train Loss: 1.5854 | Val Loss: 2.2200\n",
      "Epoch 320/100000 | Train Loss: 1.4936 | Val Loss: 2.1201\n",
      "Epoch 321/100000 | Train Loss: 1.5465 | Val Loss: 2.0285\n",
      "Epoch 322/100000 | Train Loss: 1.4069 | Val Loss: 2.2053\n",
      "Epoch 323/100000 | Train Loss: 1.2284 | Val Loss: 2.4783\n",
      "Epoch 324/100000 | Train Loss: 1.3769 | Val Loss: 2.2801\n",
      "Epoch 325/100000 | Train Loss: 1.2889 | Val Loss: 2.0590\n",
      "Epoch 326/100000 | Train Loss: 1.1710 | Val Loss: 1.9675\n",
      "Epoch 327/100000 | Train Loss: 1.2519 | Val Loss: 1.9289\n",
      "Epoch 328/100000 | Train Loss: 1.2949 | Val Loss: 1.9670\n",
      "Epoch 329/100000 | Train Loss: 1.2431 | Val Loss: 2.0615\n",
      "Epoch 330/100000 | Train Loss: 1.1976 | Val Loss: 1.9528\n",
      "Epoch 331/100000 | Train Loss: 1.1214 | Val Loss: 2.0572\n",
      "Epoch 332/100000 | Train Loss: 1.2196 | Val Loss: 2.2353\n",
      "Epoch 333/100000 | Train Loss: 1.1721 | Val Loss: 2.1296\n",
      "Epoch 334/100000 | Train Loss: 1.1439 | Val Loss: 1.8853\n",
      "Epoch 335/100000 | Train Loss: 1.1352 | Val Loss: 1.8803\n",
      "Epoch 336/100000 | Train Loss: 1.1605 | Val Loss: 1.9632\n",
      "Epoch 337/100000 | Train Loss: 1.1084 | Val Loss: 2.1014\n",
      "Epoch 338/100000 | Train Loss: 1.1332 | Val Loss: 1.9486\n",
      "Epoch 339/100000 | Train Loss: 1.0814 | Val Loss: 1.9428\n",
      "Epoch 340/100000 | Train Loss: 1.1303 | Val Loss: 1.9866\n",
      "Epoch 341/100000 | Train Loss: 1.0711 | Val Loss: 2.0612\n",
      "Epoch 342/100000 | Train Loss: 1.0985 | Val Loss: 1.9012\n",
      "Epoch 343/100000 | Train Loss: 1.0616 | Val Loss: 1.8556\n",
      "Epoch 344/100000 | Train Loss: 1.0847 | Val Loss: 1.8742\n",
      "Epoch 345/100000 | Train Loss: 1.0523 | Val Loss: 1.9937\n",
      "Epoch 346/100000 | Train Loss: 1.0591 | Val Loss: 1.9769\n",
      "Epoch 347/100000 | Train Loss: 1.0459 | Val Loss: 1.8936\n",
      "Epoch 348/100000 | Train Loss: 1.0508 | Val Loss: 1.8720\n",
      "Epoch 349/100000 | Train Loss: 1.0332 | Val Loss: 1.9142\n",
      "Epoch 350/100000 | Train Loss: 1.0306 | Val Loss: 1.8901\n",
      "Epoch 351/100000 | Train Loss: 1.0276 | Val Loss: 1.8210\n",
      "Epoch 352/100000 | Train Loss: 1.0202 | Val Loss: 1.8116\n",
      "Epoch 353/100000 | Train Loss: 1.0161 | Val Loss: 1.8727\n",
      "Epoch 354/100000 | Train Loss: 1.0041 | Val Loss: 1.9223\n",
      "Epoch 355/100000 | Train Loss: 1.0082 | Val Loss: 1.8562\n",
      "Epoch 356/100000 | Train Loss: 0.9983 | Val Loss: 1.8113\n",
      "Epoch 357/100000 | Train Loss: 0.9946 | Val Loss: 1.8178\n",
      "Epoch 358/100000 | Train Loss: 0.9859 | Val Loss: 1.8214\n",
      "Epoch 359/100000 | Train Loss: 0.9874 | Val Loss: 1.7815\n",
      "Epoch 360/100000 | Train Loss: 0.9791 | Val Loss: 1.7700\n",
      "Epoch 361/100000 | Train Loss: 0.9755 | Val Loss: 1.8054\n",
      "Epoch 362/100000 | Train Loss: 0.9684 | Val Loss: 1.8362\n",
      "Epoch 363/100000 | Train Loss: 0.9686 | Val Loss: 1.7962\n",
      "Epoch 364/100000 | Train Loss: 0.9610 | Val Loss: 1.7588\n",
      "Epoch 365/100000 | Train Loss: 0.9571 | Val Loss: 1.7544\n",
      "Epoch 366/100000 | Train Loss: 0.9522 | Val Loss: 1.7563\n",
      "Epoch 367/100000 | Train Loss: 0.9506 | Val Loss: 1.7409\n",
      "Epoch 368/100000 | Train Loss: 0.9440 | Val Loss: 1.7364\n",
      "Epoch 369/100000 | Train Loss: 0.9409 | Val Loss: 1.7534\n",
      "Epoch 370/100000 | Train Loss: 0.9364 | Val Loss: 1.7606\n",
      "Epoch 371/100000 | Train Loss: 0.9339 | Val Loss: 1.7278\n",
      "Epoch 372/100000 | Train Loss: 0.9280 | Val Loss: 1.7020\n",
      "Epoch 373/100000 | Train Loss: 0.9253 | Val Loss: 1.7021\n",
      "Epoch 374/100000 | Train Loss: 0.9214 | Val Loss: 1.7073\n",
      "Epoch 375/100000 | Train Loss: 0.9179 | Val Loss: 1.6971\n",
      "Epoch 376/100000 | Train Loss: 0.9133 | Val Loss: 1.6947\n",
      "Epoch 377/100000 | Train Loss: 0.9104 | Val Loss: 1.7050\n",
      "Epoch 378/100000 | Train Loss: 0.9068 | Val Loss: 1.6925\n",
      "Epoch 379/100000 | Train Loss: 0.9027 | Val Loss: 1.6654\n",
      "Epoch 380/100000 | Train Loss: 0.8995 | Val Loss: 1.6598\n",
      "Epoch 381/100000 | Train Loss: 0.8959 | Val Loss: 1.6689\n",
      "Epoch 382/100000 | Train Loss: 0.8927 | Val Loss: 1.6621\n",
      "Epoch 383/100000 | Train Loss: 0.8887 | Val Loss: 1.6532\n",
      "Epoch 384/100000 | Train Loss: 0.8860 | Val Loss: 1.6567\n",
      "Epoch 385/100000 | Train Loss: 0.8822 | Val Loss: 1.6490\n",
      "Epoch 386/100000 | Train Loss: 0.8789 | Val Loss: 1.6285\n",
      "Epoch 387/100000 | Train Loss: 0.8757 | Val Loss: 1.6244\n",
      "Epoch 388/100000 | Train Loss: 0.8724 | Val Loss: 1.6318\n",
      "Epoch 389/100000 | Train Loss: 0.8692 | Val Loss: 1.6258\n",
      "Epoch 390/100000 | Train Loss: 0.8658 | Val Loss: 1.6176\n",
      "Epoch 391/100000 | Train Loss: 0.8629 | Val Loss: 1.6180\n",
      "Epoch 392/100000 | Train Loss: 0.8595 | Val Loss: 1.6091\n",
      "Epoch 393/100000 | Train Loss: 0.8564 | Val Loss: 1.5953\n",
      "Epoch 394/100000 | Train Loss: 0.8534 | Val Loss: 1.5944\n",
      "Epoch 395/100000 | Train Loss: 0.8502 | Val Loss: 1.5970\n",
      "Epoch 396/100000 | Train Loss: 0.8473 | Val Loss: 1.5895\n",
      "Epoch 397/100000 | Train Loss: 0.8442 | Val Loss: 1.5831\n",
      "Epoch 398/100000 | Train Loss: 0.8413 | Val Loss: 1.5807\n",
      "Epoch 399/100000 | Train Loss: 0.8383 | Val Loss: 1.5718\n",
      "Epoch 400/100000 | Train Loss: 0.8353 | Val Loss: 1.5638\n",
      "Epoch 401/100000 | Train Loss: 0.8325 | Val Loss: 1.5654\n",
      "Epoch 402/100000 | Train Loss: 0.8295 | Val Loss: 1.5643\n",
      "Epoch 403/100000 | Train Loss: 0.8267 | Val Loss: 1.5558\n",
      "Epoch 404/100000 | Train Loss: 0.8239 | Val Loss: 1.5515\n",
      "Epoch 405/100000 | Train Loss: 0.8210 | Val Loss: 1.5475\n",
      "Epoch 406/100000 | Train Loss: 0.8182 | Val Loss: 1.5389\n",
      "Epoch 407/100000 | Train Loss: 0.8155 | Val Loss: 1.5364\n",
      "Epoch 408/100000 | Train Loss: 0.8126 | Val Loss: 1.5369\n",
      "Epoch 409/100000 | Train Loss: 0.8100 | Val Loss: 1.5303\n",
      "Epoch 410/100000 | Train Loss: 0.8072 | Val Loss: 1.5244\n",
      "Epoch 411/100000 | Train Loss: 0.8045 | Val Loss: 1.5217\n",
      "Epoch 412/100000 | Train Loss: 0.8019 | Val Loss: 1.5152\n",
      "Epoch 413/100000 | Train Loss: 0.7992 | Val Loss: 1.5108\n",
      "Epoch 414/100000 | Train Loss: 0.7966 | Val Loss: 1.5111\n",
      "Epoch 415/100000 | Train Loss: 0.7940 | Val Loss: 1.5060\n",
      "Epoch 416/100000 | Train Loss: 0.7914 | Val Loss: 1.4998\n",
      "Epoch 417/100000 | Train Loss: 0.7888 | Val Loss: 1.4970\n",
      "Epoch 418/100000 | Train Loss: 0.7862 | Val Loss: 1.4915\n",
      "Epoch 419/100000 | Train Loss: 0.7837 | Val Loss: 1.4871\n",
      "Epoch 420/100000 | Train Loss: 0.7812 | Val Loss: 1.4865\n",
      "Epoch 421/100000 | Train Loss: 0.7787 | Val Loss: 1.4816\n",
      "Epoch 422/100000 | Train Loss: 0.7762 | Val Loss: 1.4764\n",
      "Epoch 423/100000 | Train Loss: 0.7737 | Val Loss: 1.4737\n",
      "Epoch 424/100000 | Train Loss: 0.7713 | Val Loss: 1.4684\n",
      "Epoch 425/100000 | Train Loss: 0.7688 | Val Loss: 1.4646\n",
      "Epoch 426/100000 | Train Loss: 0.7664 | Val Loss: 1.4633\n",
      "Epoch 427/100000 | Train Loss: 0.7640 | Val Loss: 1.4585\n",
      "Epoch 428/100000 | Train Loss: 0.7616 | Val Loss: 1.4539\n",
      "Epoch 429/100000 | Train Loss: 0.7592 | Val Loss: 1.4515\n",
      "Epoch 430/100000 | Train Loss: 0.7569 | Val Loss: 1.4456\n",
      "Epoch 431/100000 | Train Loss: 0.7546 | Val Loss: 1.4434\n",
      "Epoch 432/100000 | Train Loss: 0.7522 | Val Loss: 1.4407\n",
      "Epoch 433/100000 | Train Loss: 0.7499 | Val Loss: 1.4359\n",
      "Epoch 434/100000 | Train Loss: 0.7476 | Val Loss: 1.4333\n",
      "Epoch 435/100000 | Train Loss: 0.7454 | Val Loss: 1.4289\n",
      "Epoch 436/100000 | Train Loss: 0.7431 | Val Loss: 1.4246\n",
      "Epoch 437/100000 | Train Loss: 0.7408 | Val Loss: 1.4227\n",
      "Epoch 438/100000 | Train Loss: 0.7386 | Val Loss: 1.4183\n",
      "Epoch 439/100000 | Train Loss: 0.7364 | Val Loss: 1.4153\n",
      "Epoch 440/100000 | Train Loss: 0.7342 | Val Loss: 1.4121\n",
      "Epoch 441/100000 | Train Loss: 0.7320 | Val Loss: 1.4075\n",
      "Epoch 442/100000 | Train Loss: 0.7298 | Val Loss: 1.4046\n",
      "Epoch 443/100000 | Train Loss: 0.7276 | Val Loss: 1.4017\n",
      "Epoch 444/100000 | Train Loss: 0.7255 | Val Loss: 1.3977\n",
      "Epoch 445/100000 | Train Loss: 0.7233 | Val Loss: 1.3954\n",
      "Epoch 446/100000 | Train Loss: 0.7212 | Val Loss: 1.3910\n",
      "Epoch 447/100000 | Train Loss: 0.7191 | Val Loss: 1.3875\n",
      "Epoch 448/100000 | Train Loss: 0.7170 | Val Loss: 1.3848\n",
      "Epoch 449/100000 | Train Loss: 0.7149 | Val Loss: 1.3812\n",
      "Epoch 450/100000 | Train Loss: 0.7128 | Val Loss: 1.3786\n",
      "Epoch 451/100000 | Train Loss: 0.7107 | Val Loss: 1.3747\n",
      "Epoch 452/100000 | Train Loss: 0.7087 | Val Loss: 1.3715\n",
      "Epoch 453/100000 | Train Loss: 0.7066 | Val Loss: 1.3685\n",
      "Epoch 454/100000 | Train Loss: 0.7046 | Val Loss: 1.3651\n",
      "Epoch 455/100000 | Train Loss: 0.7026 | Val Loss: 1.3628\n",
      "Epoch 456/100000 | Train Loss: 0.7006 | Val Loss: 1.3583\n",
      "Epoch 457/100000 | Train Loss: 0.6986 | Val Loss: 1.3563\n",
      "Epoch 458/100000 | Train Loss: 0.6966 | Val Loss: 1.3522\n",
      "Epoch 459/100000 | Train Loss: 0.6946 | Val Loss: 1.3497\n",
      "Epoch 460/100000 | Train Loss: 0.6927 | Val Loss: 1.3466\n",
      "Epoch 461/100000 | Train Loss: 0.6907 | Val Loss: 1.3432\n",
      "Epoch 462/100000 | Train Loss: 0.6888 | Val Loss: 1.3405\n",
      "Epoch 463/100000 | Train Loss: 0.6869 | Val Loss: 1.3366\n",
      "Epoch 464/100000 | Train Loss: 0.6849 | Val Loss: 1.3348\n",
      "Epoch 465/100000 | Train Loss: 0.6830 | Val Loss: 1.3308\n",
      "Epoch 466/100000 | Train Loss: 0.6811 | Val Loss: 1.3287\n",
      "Epoch 467/100000 | Train Loss: 0.6792 | Val Loss: 1.3245\n",
      "Epoch 468/100000 | Train Loss: 0.6774 | Val Loss: 1.3225\n",
      "Epoch 469/100000 | Train Loss: 0.6755 | Val Loss: 1.3187\n",
      "Epoch 470/100000 | Train Loss: 0.6736 | Val Loss: 1.3166\n",
      "Epoch 471/100000 | Train Loss: 0.6718 | Val Loss: 1.3133\n",
      "Epoch 472/100000 | Train Loss: 0.6699 | Val Loss: 1.3102\n",
      "Epoch 473/100000 | Train Loss: 0.6681 | Val Loss: 1.3074\n",
      "Epoch 474/100000 | Train Loss: 0.6663 | Val Loss: 1.3042\n",
      "Epoch 475/100000 | Train Loss: 0.6645 | Val Loss: 1.3018\n",
      "Epoch 476/100000 | Train Loss: 0.6627 | Val Loss: 1.2990\n",
      "Epoch 477/100000 | Train Loss: 0.6609 | Val Loss: 1.2955\n",
      "Epoch 478/100000 | Train Loss: 0.6591 | Val Loss: 1.2936\n",
      "Epoch 479/100000 | Train Loss: 0.6574 | Val Loss: 1.2891\n",
      "Epoch 480/100000 | Train Loss: 0.6556 | Val Loss: 1.2888\n",
      "Epoch 481/100000 | Train Loss: 0.6539 | Val Loss: 1.2831\n",
      "Epoch 482/100000 | Train Loss: 0.6522 | Val Loss: 1.2844\n",
      "Epoch 483/100000 | Train Loss: 0.6506 | Val Loss: 1.2758\n",
      "Epoch 484/100000 | Train Loss: 0.6493 | Val Loss: 1.2839\n",
      "Epoch 485/100000 | Train Loss: 0.6491 | Val Loss: 1.2683\n",
      "Epoch 486/100000 | Train Loss: 0.6512 | Val Loss: 1.3064\n",
      "Epoch 487/100000 | Train Loss: 0.6640 | Val Loss: 1.2776\n",
      "Epoch 488/100000 | Train Loss: 0.6863 | Val Loss: 1.4249\n",
      "Epoch 489/100000 | Train Loss: 0.7909 | Val Loss: 1.2891\n",
      "Epoch 490/100000 | Train Loss: 0.7201 | Val Loss: 1.2468\n",
      "Epoch 491/100000 | Train Loss: 0.6554 | Val Loss: 1.3286\n",
      "Epoch 492/100000 | Train Loss: 0.6502 | Val Loss: 1.3231\n",
      "Epoch 493/100000 | Train Loss: 0.7397 | Val Loss: 1.7061\n",
      "Epoch 494/100000 | Train Loss: 1.0657 | Val Loss: 1.4078\n",
      "Epoch 495/100000 | Train Loss: 0.9040 | Val Loss: 1.4283\n",
      "Epoch 496/100000 | Train Loss: 0.9047 | Val Loss: 1.6196\n",
      "Epoch 497/100000 | Train Loss: 0.8024 | Val Loss: 1.6484\n",
      "Epoch 498/100000 | Train Loss: 0.9127 | Val Loss: 1.8317\n",
      "Epoch 499/100000 | Train Loss: 1.0268 | Val Loss: 1.5262\n",
      "Epoch 500/100000 | Train Loss: 0.9611 | Val Loss: 1.5739\n",
      "Epoch 501/100000 | Train Loss: 1.0637 | Val Loss: 1.4154\n",
      "Epoch 502/100000 | Train Loss: 0.7914 | Val Loss: 1.7014\n",
      "Epoch 503/100000 | Train Loss: 0.8179 | Val Loss: 2.0795\n",
      "Epoch 504/100000 | Train Loss: 1.5243 | Val Loss: 2.3157\n",
      "Epoch 505/100000 | Train Loss: 1.6632 | Val Loss: 1.5228\n",
      "Epoch 506/100000 | Train Loss: 1.1732 | Val Loss: 2.0566\n",
      "Epoch 507/100000 | Train Loss: 1.6441 | Val Loss: 1.6596\n",
      "Epoch 508/100000 | Train Loss: 1.3666 | Val Loss: 1.6100\n",
      "Epoch 509/100000 | Train Loss: 1.2495 | Val Loss: 1.8619\n",
      "Epoch 510/100000 | Train Loss: 1.4247 | Val Loss: 1.3143\n",
      "Epoch 511/100000 | Train Loss: 1.0547 | Val Loss: 1.6705\n",
      "Epoch 512/100000 | Train Loss: 1.2952 | Val Loss: 1.4736\n",
      "Epoch 513/100000 | Train Loss: 1.1002 | Val Loss: 1.4580\n",
      "Epoch 514/100000 | Train Loss: 1.0337 | Val Loss: 1.7005\n",
      "Epoch 515/100000 | Train Loss: 1.1582 | Val Loss: 1.3730\n",
      "Epoch 516/100000 | Train Loss: 0.9312 | Val Loss: 1.4921\n",
      "Epoch 517/100000 | Train Loss: 1.0794 | Val Loss: 1.3745\n",
      "Epoch 518/100000 | Train Loss: 0.9777 | Val Loss: 1.4885\n",
      "Epoch 519/100000 | Train Loss: 0.9255 | Val Loss: 1.7449\n",
      "Epoch 520/100000 | Train Loss: 1.0143 | Val Loss: 1.4322\n",
      "Epoch 521/100000 | Train Loss: 0.8671 | Val Loss: 1.4085\n",
      "Epoch 522/100000 | Train Loss: 0.9492 | Val Loss: 1.3987\n",
      "Epoch 523/100000 | Train Loss: 0.9014 | Val Loss: 1.5376\n",
      "Epoch 524/100000 | Train Loss: 0.8551 | Val Loss: 1.7204\n",
      "Epoch 525/100000 | Train Loss: 0.9120 | Val Loss: 1.5054\n",
      "Epoch 526/100000 | Train Loss: 0.8330 | Val Loss: 1.4213\n",
      "Epoch 527/100000 | Train Loss: 0.8590 | Val Loss: 1.4442\n",
      "Epoch 528/100000 | Train Loss: 0.8548 | Val Loss: 1.5254\n",
      "Epoch 529/100000 | Train Loss: 0.8059 | Val Loss: 1.6888\n",
      "Epoch 530/100000 | Train Loss: 0.8463 | Val Loss: 1.5705\n",
      "Epoch 531/100000 | Train Loss: 0.7999 | Val Loss: 1.4619\n",
      "Epoch 532/100000 | Train Loss: 0.8058 | Val Loss: 1.4490\n",
      "Epoch 533/100000 | Train Loss: 0.8103 | Val Loss: 1.5178\n",
      "Epoch 534/100000 | Train Loss: 0.7780 | Val Loss: 1.6640\n",
      "Epoch 535/100000 | Train Loss: 0.7985 | Val Loss: 1.6043\n",
      "Epoch 536/100000 | Train Loss: 0.7777 | Val Loss: 1.4781\n",
      "Epoch 537/100000 | Train Loss: 0.7731 | Val Loss: 1.4566\n",
      "Epoch 538/100000 | Train Loss: 0.7809 | Val Loss: 1.5239\n",
      "Epoch 539/100000 | Train Loss: 0.7573 | Val Loss: 1.6411\n",
      "Epoch 540/100000 | Train Loss: 0.7697 | Val Loss: 1.5876\n",
      "Epoch 541/100000 | Train Loss: 0.7557 | Val Loss: 1.4820\n",
      "Epoch 542/100000 | Train Loss: 0.7520 | Val Loss: 1.4650\n",
      "Epoch 543/100000 | Train Loss: 0.7549 | Val Loss: 1.5221\n",
      "Epoch 544/100000 | Train Loss: 0.7411 | Val Loss: 1.5956\n",
      "Epoch 545/100000 | Train Loss: 0.7461 | Val Loss: 1.5572\n",
      "Epoch 546/100000 | Train Loss: 0.7384 | Val Loss: 1.4802\n",
      "Epoch 547/100000 | Train Loss: 0.7339 | Val Loss: 1.4635\n",
      "Epoch 548/100000 | Train Loss: 0.7357 | Val Loss: 1.4984\n",
      "Epoch 549/100000 | Train Loss: 0.7255 | Val Loss: 1.5527\n",
      "Epoch 550/100000 | Train Loss: 0.7284 | Val Loss: 1.5263\n",
      "Epoch 551/100000 | Train Loss: 0.7217 | Val Loss: 1.4647\n",
      "Epoch 552/100000 | Train Loss: 0.7191 | Val Loss: 1.4445\n",
      "Epoch 553/100000 | Train Loss: 0.7184 | Val Loss: 1.4779\n",
      "Epoch 554/100000 | Train Loss: 0.7120 | Val Loss: 1.5201\n",
      "Epoch 555/100000 | Train Loss: 0.7130 | Val Loss: 1.4910\n",
      "Epoch 556/100000 | Train Loss: 0.7079 | Val Loss: 1.4396\n",
      "Epoch 557/100000 | Train Loss: 0.7062 | Val Loss: 1.4324\n",
      "Epoch 558/100000 | Train Loss: 0.7044 | Val Loss: 1.4661\n",
      "Epoch 559/100000 | Train Loss: 0.7003 | Val Loss: 1.4869\n",
      "Epoch 560/100000 | Train Loss: 0.6999 | Val Loss: 1.4557\n",
      "Epoch 561/100000 | Train Loss: 0.6958 | Val Loss: 1.4237\n",
      "Epoch 562/100000 | Train Loss: 0.6945 | Val Loss: 1.4249\n",
      "Epoch 563/100000 | Train Loss: 0.6922 | Val Loss: 1.4479\n",
      "Epoch 564/100000 | Train Loss: 0.6894 | Val Loss: 1.4563\n",
      "Epoch 565/100000 | Train Loss: 0.6884 | Val Loss: 1.4315\n",
      "Epoch 566/100000 | Train Loss: 0.6851 | Val Loss: 1.4092\n",
      "Epoch 567/100000 | Train Loss: 0.6841 | Val Loss: 1.4118\n",
      "Epoch 568/100000 | Train Loss: 0.6814 | Val Loss: 1.4317\n",
      "Epoch 569/100000 | Train Loss: 0.6796 | Val Loss: 1.4327\n",
      "Epoch 570/100000 | Train Loss: 0.6779 | Val Loss: 1.4082\n",
      "Epoch 571/100000 | Train Loss: 0.6754 | Val Loss: 1.3931\n",
      "Epoch 572/100000 | Train Loss: 0.6742 | Val Loss: 1.4038\n",
      "Epoch 573/100000 | Train Loss: 0.6717 | Val Loss: 1.4189\n",
      "Epoch 574/100000 | Train Loss: 0.6703 | Val Loss: 1.4084\n",
      "Epoch 575/100000 | Train Loss: 0.6682 | Val Loss: 1.3886\n",
      "Epoch 576/100000 | Train Loss: 0.6665 | Val Loss: 1.3866\n",
      "Epoch 577/100000 | Train Loss: 0.6648 | Val Loss: 1.3986\n",
      "Epoch 578/100000 | Train Loss: 0.6628 | Val Loss: 1.4025\n",
      "Epoch 579/100000 | Train Loss: 0.6613 | Val Loss: 1.3901\n",
      "Epoch 580/100000 | Train Loss: 0.6593 | Val Loss: 1.3791\n",
      "Epoch 581/100000 | Train Loss: 0.6578 | Val Loss: 1.3821\n",
      "Epoch 582/100000 | Train Loss: 0.6559 | Val Loss: 1.3907\n",
      "Epoch 583/100000 | Train Loss: 0.6543 | Val Loss: 1.3891\n",
      "Epoch 584/100000 | Train Loss: 0.6526 | Val Loss: 1.3769\n",
      "Epoch 585/100000 | Train Loss: 0.6509 | Val Loss: 1.3710\n",
      "Epoch 586/100000 | Train Loss: 0.6493 | Val Loss: 1.3777\n",
      "Epoch 587/100000 | Train Loss: 0.6476 | Val Loss: 1.3827\n",
      "Epoch 588/100000 | Train Loss: 0.6461 | Val Loss: 1.3743\n",
      "Epoch 589/100000 | Train Loss: 0.6443 | Val Loss: 1.3645\n",
      "Early stopping at epoch 590\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "save_path = get_next_model_filename(prefix='PIRN_nonuni')\n",
    "trainer.fit(train_loader, val_loader, num_epochs=100000, early_stop_patience=100, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a73b2-f39f-4923-8ab9-822f911197c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7927ec45-8061-4c8c-b6a0-42a612b9e0a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81f3cd966fb409198f953b6728d5c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100000 | Train Loss: 637353.3125 | Val Loss: 16835722.0000\n",
      "Epoch 2/100000 | Train Loss: 635931.3750 | Val Loss: 16844248.0000\n",
      "Epoch 3/100000 | Train Loss: 634812.1250 | Val Loss: 16839936.0000\n",
      "Epoch 4/100000 | Train Loss: 633235.1250 | Val Loss: 16825800.0000\n",
      "Epoch 5/100000 | Train Loss: 631554.5000 | Val Loss: 16821710.0000\n",
      "Epoch 6/100000 | Train Loss: 630906.9375 | Val Loss: 16811430.0000\n",
      "Epoch 7/100000 | Train Loss: 629300.5000 | Val Loss: 16816916.0000\n",
      "Epoch 8/100000 | Train Loss: 628817.1250 | Val Loss: 16806392.0000\n",
      "Epoch 9/100000 | Train Loss: 626850.1250 | Val Loss: 16778140.0000\n",
      "Epoch 10/100000 | Train Loss: 626321.4375 | Val Loss: 16770414.0000\n",
      "Epoch 11/100000 | Train Loss: 624780.5000 | Val Loss: 16770861.0000\n",
      "Epoch 12/100000 | Train Loss: 623517.5000 | Val Loss: 16761947.0000\n",
      "Epoch 13/100000 | Train Loss: 621774.2500 | Val Loss: 16744682.0000\n",
      "Epoch 14/100000 | Train Loss: 620697.9375 | Val Loss: 16738922.0000\n",
      "Epoch 15/100000 | Train Loss: 619687.0000 | Val Loss: 16749329.0000\n",
      "Epoch 16/100000 | Train Loss: 618241.2500 | Val Loss: 16748306.0000\n",
      "Epoch 17/100000 | Train Loss: 617019.2500 | Val Loss: 16737684.0000\n",
      "Epoch 18/100000 | Train Loss: 615078.6875 | Val Loss: 16721878.0000\n",
      "Epoch 19/100000 | Train Loss: 615580.2500 | Val Loss: 16704817.0000\n",
      "Epoch 20/100000 | Train Loss: 612616.0000 | Val Loss: 16708320.0000\n",
      "Epoch 21/100000 | Train Loss: 612242.4375 | Val Loss: 16702520.0000\n",
      "Epoch 22/100000 | Train Loss: 610701.7500 | Val Loss: 16689530.0000\n",
      "Epoch 23/100000 | Train Loss: 609271.1875 | Val Loss: 16684707.0000\n",
      "Epoch 24/100000 | Train Loss: 609644.1250 | Val Loss: 16682586.0000\n",
      "Epoch 25/100000 | Train Loss: 606235.6250 | Val Loss: 16705184.0000\n",
      "Epoch 26/100000 | Train Loss: 606638.1250 | Val Loss: 16689612.0000\n",
      "Epoch 27/100000 | Train Loss: 603883.0000 | Val Loss: 16664972.0000\n",
      "Epoch 28/100000 | Train Loss: 603406.8125 | Val Loss: 16670652.0000\n",
      "Epoch 29/100000 | Train Loss: 603505.6250 | Val Loss: 16677933.0000\n",
      "Epoch 30/100000 | Train Loss: 601394.3750 | Val Loss: 16674292.0000\n",
      "Epoch 31/100000 | Train Loss: 600713.7500 | Val Loss: 16656987.0000\n",
      "Epoch 32/100000 | Train Loss: 597864.6250 | Val Loss: 16641722.0000\n",
      "Epoch 33/100000 | Train Loss: 599136.5000 | Val Loss: 16643713.0000\n",
      "Epoch 34/100000 | Train Loss: 595128.6875 | Val Loss: 16638616.0000\n",
      "Epoch 35/100000 | Train Loss: 595610.3750 | Val Loss: 16638544.0000\n",
      "Epoch 36/100000 | Train Loss: 593950.8125 | Val Loss: 16612212.0000\n",
      "Epoch 37/100000 | Train Loss: 592845.0000 | Val Loss: 16617604.0000\n",
      "Epoch 38/100000 | Train Loss: 591650.1875 | Val Loss: 16617488.0000\n",
      "Epoch 39/100000 | Train Loss: 590100.6875 | Val Loss: 16619570.0000\n",
      "Epoch 40/100000 | Train Loss: 589147.3750 | Val Loss: 16600325.0000\n",
      "Epoch 41/100000 | Train Loss: 587037.6250 | Val Loss: 16593221.0000\n",
      "Epoch 42/100000 | Train Loss: 587308.6250 | Val Loss: 16583914.0000\n",
      "Epoch 43/100000 | Train Loss: 585546.6250 | Val Loss: 16590678.0000\n",
      "Epoch 44/100000 | Train Loss: 584803.0625 | Val Loss: 16592734.0000\n",
      "Epoch 45/100000 | Train Loss: 583796.1250 | Val Loss: 16580704.0000\n",
      "Epoch 46/100000 | Train Loss: 581665.1250 | Val Loss: 16568925.0000\n",
      "Epoch 47/100000 | Train Loss: 581552.0000 | Val Loss: 16578186.0000\n",
      "Epoch 48/100000 | Train Loss: 578857.6250 | Val Loss: 16582981.0000\n",
      "Epoch 49/100000 | Train Loss: 578194.6250 | Val Loss: 16582878.0000\n",
      "Epoch 50/100000 | Train Loss: 577066.1250 | Val Loss: 16577147.0000\n",
      "Epoch 51/100000 | Train Loss: 575960.2500 | Val Loss: 16566328.0000\n",
      "Epoch 52/100000 | Train Loss: 574890.8750 | Val Loss: 16580902.0000\n",
      "Epoch 53/100000 | Train Loss: 573201.8750 | Val Loss: 16572608.0000\n",
      "Epoch 54/100000 | Train Loss: 572347.3750 | Val Loss: 16565569.0000\n",
      "Epoch 55/100000 | Train Loss: 571034.9375 | Val Loss: 16554622.0000\n",
      "Epoch 56/100000 | Train Loss: 570199.6875 | Val Loss: 16554318.0000\n",
      "Epoch 57/100000 | Train Loss: 568966.5625 | Val Loss: 16547359.0000\n",
      "Epoch 58/100000 | Train Loss: 567722.7500 | Val Loss: 16547946.0000\n",
      "Epoch 59/100000 | Train Loss: 566950.6250 | Val Loss: 16547090.0000\n",
      "Epoch 60/100000 | Train Loss: 566015.3750 | Val Loss: 16542708.0000\n",
      "Epoch 61/100000 | Train Loss: 564511.1875 | Val Loss: 16531514.0000\n",
      "Epoch 62/100000 | Train Loss: 564134.7500 | Val Loss: 16538132.0000\n",
      "Epoch 63/100000 | Train Loss: 563080.0625 | Val Loss: 16535626.0000\n",
      "Epoch 64/100000 | Train Loss: 562089.5000 | Val Loss: 16517830.0000\n",
      "Epoch 65/100000 | Train Loss: 561124.3750 | Val Loss: 16520968.0000\n",
      "Epoch 66/100000 | Train Loss: 559095.8750 | Val Loss: 16510798.0000\n",
      "Epoch 67/100000 | Train Loss: 558164.0000 | Val Loss: 16496980.0000\n",
      "Epoch 68/100000 | Train Loss: 557338.6250 | Val Loss: 16483443.0000\n",
      "Epoch 69/100000 | Train Loss: 556022.6250 | Val Loss: 16481686.0000\n",
      "Epoch 70/100000 | Train Loss: 554972.6250 | Val Loss: 16471886.0000\n",
      "Epoch 71/100000 | Train Loss: 553625.3750 | Val Loss: 16471116.0000\n",
      "Epoch 72/100000 | Train Loss: 552438.7500 | Val Loss: 16470926.0000\n",
      "Epoch 73/100000 | Train Loss: 551373.3750 | Val Loss: 16464826.0000\n",
      "Epoch 74/100000 | Train Loss: 550421.9375 | Val Loss: 16467152.0000\n",
      "Epoch 75/100000 | Train Loss: 549394.8125 | Val Loss: 16446891.0000\n",
      "Epoch 76/100000 | Train Loss: 548702.6875 | Val Loss: 16455050.0000\n",
      "Epoch 77/100000 | Train Loss: 547619.1250 | Val Loss: 16463093.0000\n",
      "Epoch 78/100000 | Train Loss: 546690.6250 | Val Loss: 16467006.0000\n",
      "Epoch 79/100000 | Train Loss: 546381.0000 | Val Loss: 16459638.0000\n",
      "Epoch 80/100000 | Train Loss: 544456.5000 | Val Loss: 16447526.0000\n",
      "Epoch 81/100000 | Train Loss: 544472.5000 | Val Loss: 16460432.0000\n",
      "Epoch 82/100000 | Train Loss: 542503.0625 | Val Loss: 16452210.0000\n",
      "Epoch 83/100000 | Train Loss: 541650.0625 | Val Loss: 16452287.0000\n",
      "Epoch 84/100000 | Train Loss: 540349.6250 | Val Loss: 16444845.0000\n",
      "Epoch 85/100000 | Train Loss: 539228.1250 | Val Loss: 16450214.0000\n",
      "Epoch 86/100000 | Train Loss: 538691.0000 | Val Loss: 16449150.0000\n",
      "Epoch 87/100000 | Train Loss: 537304.0625 | Val Loss: 16445547.0000\n",
      "Epoch 88/100000 | Train Loss: 536653.6250 | Val Loss: 16428936.0000\n",
      "Epoch 89/100000 | Train Loss: 535157.7500 | Val Loss: 16426576.0000\n",
      "Epoch 90/100000 | Train Loss: 534669.7500 | Val Loss: 16432142.0000\n",
      "Epoch 91/100000 | Train Loss: 533429.6875 | Val Loss: 16439610.0000\n",
      "Epoch 92/100000 | Train Loss: 532272.8750 | Val Loss: 16435462.0000\n",
      "Epoch 93/100000 | Train Loss: 531335.8750 | Val Loss: 16439450.0000\n",
      "Epoch 94/100000 | Train Loss: 530436.1250 | Val Loss: 16436834.0000\n",
      "Epoch 95/100000 | Train Loss: 529381.1875 | Val Loss: 16425866.0000\n",
      "Epoch 96/100000 | Train Loss: 529133.2500 | Val Loss: 16431490.0000\n",
      "Epoch 97/100000 | Train Loss: 527954.6250 | Val Loss: 16433726.0000\n",
      "Epoch 98/100000 | Train Loss: 527463.7500 | Val Loss: 16421465.0000\n",
      "Epoch 99/100000 | Train Loss: 525999.0000 | Val Loss: 16415070.0000\n",
      "Epoch 100/100000 | Train Loss: 525464.8750 | Val Loss: 16419328.0000\n",
      "Epoch 101/100000 | Train Loss: 523894.9062 | Val Loss: 16419262.0000\n",
      "Epoch 102/100000 | Train Loss: 523329.2500 | Val Loss: 16403556.0000\n",
      "Epoch 103/100000 | Train Loss: 521589.9375 | Val Loss: 16390308.0000\n",
      "Epoch 104/100000 | Train Loss: 523084.1875 | Val Loss: 16404258.0000\n",
      "Epoch 105/100000 | Train Loss: 519870.7500 | Val Loss: 16407378.0000\n",
      "Epoch 106/100000 | Train Loss: 521547.8438 | Val Loss: 16382098.0000\n",
      "Epoch 107/100000 | Train Loss: 518373.1875 | Val Loss: 16385046.0000\n",
      "Epoch 108/100000 | Train Loss: 518762.0000 | Val Loss: 16403978.0000\n",
      "Epoch 109/100000 | Train Loss: 516392.0000 | Val Loss: 16400822.0000\n",
      "Epoch 110/100000 | Train Loss: 517520.6250 | Val Loss: 16377010.0000\n",
      "Epoch 111/100000 | Train Loss: 514441.8750 | Val Loss: 16377296.0000\n",
      "Epoch 112/100000 | Train Loss: 514617.3750 | Val Loss: 16384265.0000\n",
      "Epoch 113/100000 | Train Loss: 512287.5000 | Val Loss: 16388852.0000\n",
      "Epoch 114/100000 | Train Loss: 512684.5000 | Val Loss: 16383160.0000\n",
      "Epoch 115/100000 | Train Loss: 510568.5312 | Val Loss: 16360213.0000\n",
      "Epoch 116/100000 | Train Loss: 510596.2500 | Val Loss: 16351611.0000\n",
      "Epoch 117/100000 | Train Loss: 509360.6875 | Val Loss: 16359634.0000\n",
      "Epoch 118/100000 | Train Loss: 507996.5625 | Val Loss: 16356822.0000\n",
      "Epoch 119/100000 | Train Loss: 507918.8125 | Val Loss: 16344220.0000\n",
      "Epoch 120/100000 | Train Loss: 505723.4375 | Val Loss: 16332861.0000\n",
      "Epoch 121/100000 | Train Loss: 506004.8438 | Val Loss: 16334188.0000\n",
      "Epoch 122/100000 | Train Loss: 504250.7500 | Val Loss: 16331096.0000\n",
      "Epoch 123/100000 | Train Loss: 503226.2188 | Val Loss: 16335552.0000\n",
      "Epoch 124/100000 | Train Loss: 503099.9375 | Val Loss: 16329673.0000\n",
      "Epoch 125/100000 | Train Loss: 501920.9375 | Val Loss: 16320838.0000\n",
      "Epoch 126/100000 | Train Loss: 500883.7812 | Val Loss: 16311749.0000\n",
      "Epoch 127/100000 | Train Loss: 500883.7500 | Val Loss: 16325156.0000\n",
      "Epoch 128/100000 | Train Loss: 499227.6250 | Val Loss: 16323820.0000\n",
      "Epoch 129/100000 | Train Loss: 499106.3750 | Val Loss: 16313183.0000\n",
      "Epoch 130/100000 | Train Loss: 497121.3438 | Val Loss: 16313918.0000\n",
      "Epoch 131/100000 | Train Loss: 497779.2500 | Val Loss: 16305048.0000\n",
      "Epoch 132/100000 | Train Loss: 495110.7812 | Val Loss: 16309962.0000\n",
      "Epoch 133/100000 | Train Loss: 495726.5000 | Val Loss: 16298058.0000\n",
      "Epoch 134/100000 | Train Loss: 493613.6250 | Val Loss: 16286868.0000\n",
      "Epoch 135/100000 | Train Loss: 492865.0625 | Val Loss: 16279590.0000\n",
      "Epoch 136/100000 | Train Loss: 492354.9688 | Val Loss: 16272339.0000\n",
      "Epoch 137/100000 | Train Loss: 491020.0000 | Val Loss: 16286287.0000\n",
      "Epoch 138/100000 | Train Loss: 491340.5938 | Val Loss: 16273814.0000\n",
      "Epoch 139/100000 | Train Loss: 489139.8125 | Val Loss: 16269961.0000\n",
      "Epoch 140/100000 | Train Loss: 489899.3125 | Val Loss: 16267140.0000\n",
      "Epoch 141/100000 | Train Loss: 487752.8750 | Val Loss: 16275112.0000\n",
      "Epoch 142/100000 | Train Loss: 488187.5312 | Val Loss: 16260312.0000\n",
      "Epoch 143/100000 | Train Loss: 486266.8438 | Val Loss: 16261321.0000\n",
      "Epoch 144/100000 | Train Loss: 485808.7188 | Val Loss: 16267912.0000\n",
      "Epoch 145/100000 | Train Loss: 484677.5000 | Val Loss: 16272918.0000\n",
      "Epoch 146/100000 | Train Loss: 484105.4375 | Val Loss: 16265744.0000\n",
      "Epoch 147/100000 | Train Loss: 483396.5625 | Val Loss: 16250470.0000\n",
      "Epoch 148/100000 | Train Loss: 482880.6875 | Val Loss: 16260192.0000\n",
      "Epoch 149/100000 | Train Loss: 480701.8125 | Val Loss: 16261311.0000\n",
      "Epoch 150/100000 | Train Loss: 480516.8125 | Val Loss: 16263038.0000\n",
      "Epoch 151/100000 | Train Loss: 479435.0625 | Val Loss: 16253310.0000\n",
      "Epoch 152/100000 | Train Loss: 479017.9688 | Val Loss: 16259696.0000\n",
      "Epoch 153/100000 | Train Loss: 477648.5938 | Val Loss: 16262646.0000\n",
      "Epoch 154/100000 | Train Loss: 477012.8125 | Val Loss: 16265696.0000\n",
      "Epoch 155/100000 | Train Loss: 476701.0625 | Val Loss: 16243002.0000\n",
      "Epoch 156/100000 | Train Loss: 475440.8438 | Val Loss: 16241780.0000\n",
      "Epoch 157/100000 | Train Loss: 475615.7812 | Val Loss: 16248432.0000\n",
      "Epoch 158/100000 | Train Loss: 473938.9375 | Val Loss: 16258622.0000\n",
      "Epoch 159/100000 | Train Loss: 473789.0000 | Val Loss: 16252062.0000\n",
      "Epoch 160/100000 | Train Loss: 472308.4375 | Val Loss: 16247919.0000\n",
      "Epoch 161/100000 | Train Loss: 471832.7500 | Val Loss: 16249892.0000\n",
      "Epoch 162/100000 | Train Loss: 471113.5312 | Val Loss: 16252271.0000\n",
      "Epoch 163/100000 | Train Loss: 469925.4062 | Val Loss: 16260311.0000\n",
      "Epoch 164/100000 | Train Loss: 469753.6250 | Val Loss: 16250716.0000\n",
      "Epoch 165/100000 | Train Loss: 468495.2812 | Val Loss: 16221439.0000\n",
      "Epoch 166/100000 | Train Loss: 468921.3125 | Val Loss: 16238512.0000\n",
      "Epoch 167/100000 | Train Loss: 466911.4062 | Val Loss: 16249252.0000\n",
      "Epoch 168/100000 | Train Loss: 467089.7500 | Val Loss: 16236514.0000\n",
      "Epoch 169/100000 | Train Loss: 465265.9688 | Val Loss: 16224531.0000\n",
      "Epoch 170/100000 | Train Loss: 466338.4375 | Val Loss: 16234180.0000\n",
      "Epoch 171/100000 | Train Loss: 464071.6250 | Val Loss: 16244675.0000\n",
      "Epoch 172/100000 | Train Loss: 464370.7188 | Val Loss: 16222899.0000\n",
      "Epoch 173/100000 | Train Loss: 462329.9688 | Val Loss: 16213202.0000\n",
      "Epoch 174/100000 | Train Loss: 463488.6250 | Val Loss: 16233644.0000\n",
      "Epoch 175/100000 | Train Loss: 461719.0625 | Val Loss: 16222440.0000\n",
      "Epoch 176/100000 | Train Loss: 460141.3750 | Val Loss: 16212974.0000\n",
      "Epoch 177/100000 | Train Loss: 459311.8750 | Val Loss: 16213667.0000\n",
      "Epoch 178/100000 | Train Loss: 458421.1250 | Val Loss: 16217942.0000\n",
      "Epoch 179/100000 | Train Loss: 457724.3438 | Val Loss: 16217379.0000\n",
      "Epoch 180/100000 | Train Loss: 456998.1250 | Val Loss: 16211356.0000\n",
      "Epoch 181/100000 | Train Loss: 456377.1250 | Val Loss: 16215946.0000\n",
      "Epoch 182/100000 | Train Loss: 455522.8125 | Val Loss: 16217952.0000\n",
      "Epoch 183/100000 | Train Loss: 455064.4375 | Val Loss: 16219070.0000\n",
      "Epoch 184/100000 | Train Loss: 453913.4375 | Val Loss: 16223420.0000\n",
      "Epoch 185/100000 | Train Loss: 453762.3750 | Val Loss: 16212336.0000\n",
      "Epoch 186/100000 | Train Loss: 452791.9375 | Val Loss: 16209945.0000\n",
      "Epoch 187/100000 | Train Loss: 452302.7812 | Val Loss: 16218423.0000\n",
      "Epoch 188/100000 | Train Loss: 451681.6250 | Val Loss: 16210606.0000\n",
      "Epoch 189/100000 | Train Loss: 450726.7812 | Val Loss: 16198630.0000\n",
      "Epoch 190/100000 | Train Loss: 450058.5938 | Val Loss: 16204548.0000\n",
      "Epoch 191/100000 | Train Loss: 449173.0000 | Val Loss: 16202697.0000\n",
      "Epoch 192/100000 | Train Loss: 448442.1562 | Val Loss: 16192108.0000\n",
      "Epoch 193/100000 | Train Loss: 447758.9375 | Val Loss: 16185269.0000\n",
      "Epoch 194/100000 | Train Loss: 446802.1562 | Val Loss: 16183025.0000\n",
      "Epoch 195/100000 | Train Loss: 446362.0000 | Val Loss: 16183317.0000\n",
      "Epoch 196/100000 | Train Loss: 445508.7188 | Val Loss: 16185882.0000\n",
      "Epoch 197/100000 | Train Loss: 445254.8125 | Val Loss: 16169648.0000\n",
      "Epoch 198/100000 | Train Loss: 444112.9688 | Val Loss: 16167580.0000\n",
      "Epoch 199/100000 | Train Loss: 443278.0625 | Val Loss: 16175548.0000\n",
      "Epoch 200/100000 | Train Loss: 443057.5000 | Val Loss: 16171410.0000\n",
      "Epoch 201/100000 | Train Loss: 441881.8438 | Val Loss: 16161653.0000\n",
      "Epoch 202/100000 | Train Loss: 441819.2500 | Val Loss: 16161950.0000\n",
      "Epoch 203/100000 | Train Loss: 440669.9375 | Val Loss: 16167308.0000\n",
      "Epoch 204/100000 | Train Loss: 440057.0312 | Val Loss: 16152588.0000\n",
      "Epoch 205/100000 | Train Loss: 439336.9375 | Val Loss: 16158440.0000\n",
      "Epoch 206/100000 | Train Loss: 438703.5938 | Val Loss: 16150166.0000\n",
      "Epoch 207/100000 | Train Loss: 438047.6250 | Val Loss: 16154231.0000\n",
      "Epoch 208/100000 | Train Loss: 437821.6562 | Val Loss: 16142017.0000\n",
      "Epoch 209/100000 | Train Loss: 436496.5000 | Val Loss: 16136220.0000\n",
      "Epoch 210/100000 | Train Loss: 436328.8125 | Val Loss: 16136707.0000\n",
      "Epoch 211/100000 | Train Loss: 435430.4375 | Val Loss: 16139441.0000\n",
      "Epoch 212/100000 | Train Loss: 434764.3750 | Val Loss: 16122843.0000\n",
      "Epoch 213/100000 | Train Loss: 433977.2500 | Val Loss: 16119064.0000\n",
      "Epoch 214/100000 | Train Loss: 433371.3125 | Val Loss: 16124208.0000\n",
      "Epoch 215/100000 | Train Loss: 432599.6875 | Val Loss: 16114349.0000\n",
      "Epoch 216/100000 | Train Loss: 431896.1250 | Val Loss: 16121392.0000\n",
      "Epoch 217/100000 | Train Loss: 431360.3750 | Val Loss: 16127319.0000\n",
      "Epoch 218/100000 | Train Loss: 430763.0625 | Val Loss: 16122646.0000\n",
      "Epoch 219/100000 | Train Loss: 429994.1875 | Val Loss: 16125454.0000\n",
      "Epoch 220/100000 | Train Loss: 429197.6875 | Val Loss: 16134843.0000\n",
      "Epoch 221/100000 | Train Loss: 428532.9375 | Val Loss: 16140116.0000\n",
      "Epoch 222/100000 | Train Loss: 428086.0625 | Val Loss: 16146524.0000\n",
      "Epoch 223/100000 | Train Loss: 427257.8125 | Val Loss: 16134446.0000\n",
      "Epoch 224/100000 | Train Loss: 427155.8125 | Val Loss: 16132906.0000\n",
      "Epoch 225/100000 | Train Loss: 425943.9375 | Val Loss: 16143464.0000\n",
      "Epoch 226/100000 | Train Loss: 425254.9375 | Val Loss: 16154300.0000\n",
      "Epoch 227/100000 | Train Loss: 425846.3125 | Val Loss: 16136018.0000\n",
      "Epoch 228/100000 | Train Loss: 424136.0625 | Val Loss: 16121398.0000\n",
      "Epoch 229/100000 | Train Loss: 424574.5000 | Val Loss: 16144431.0000\n",
      "Epoch 230/100000 | Train Loss: 422955.3438 | Val Loss: 16144228.0000\n",
      "Epoch 231/100000 | Train Loss: 423424.2188 | Val Loss: 16120378.0000\n",
      "Epoch 232/100000 | Train Loss: 422503.1250 | Val Loss: 16125295.0000\n",
      "Epoch 233/100000 | Train Loss: 421121.8750 | Val Loss: 16130555.0000\n",
      "Epoch 234/100000 | Train Loss: 420735.8125 | Val Loss: 16130809.0000\n",
      "Epoch 235/100000 | Train Loss: 420698.6875 | Val Loss: 16112813.0000\n",
      "Epoch 236/100000 | Train Loss: 419778.8125 | Val Loss: 16110965.0000\n",
      "Epoch 237/100000 | Train Loss: 419184.5000 | Val Loss: 16122094.0000\n",
      "Epoch 238/100000 | Train Loss: 418179.6562 | Val Loss: 16125736.0000\n",
      "Epoch 239/100000 | Train Loss: 418029.0938 | Val Loss: 16110902.0000\n",
      "Epoch 240/100000 | Train Loss: 416795.8125 | Val Loss: 16113266.0000\n",
      "Epoch 241/100000 | Train Loss: 416020.1250 | Val Loss: 16109261.0000\n",
      "Epoch 242/100000 | Train Loss: 415578.7500 | Val Loss: 16104118.0000\n",
      "Epoch 243/100000 | Train Loss: 414680.5625 | Val Loss: 16105388.0000\n",
      "Epoch 244/100000 | Train Loss: 414308.7812 | Val Loss: 16108123.0000\n",
      "Epoch 245/100000 | Train Loss: 413541.7188 | Val Loss: 16109104.0000\n",
      "Epoch 246/100000 | Train Loss: 413008.8125 | Val Loss: 16106188.0000\n",
      "Epoch 247/100000 | Train Loss: 412215.0000 | Val Loss: 16105941.0000\n",
      "Epoch 248/100000 | Train Loss: 411622.6250 | Val Loss: 16103382.0000\n",
      "Epoch 249/100000 | Train Loss: 411588.6250 | Val Loss: 16106320.0000\n",
      "Epoch 250/100000 | Train Loss: 410624.3125 | Val Loss: 16097574.0000\n",
      "Epoch 251/100000 | Train Loss: 409949.7812 | Val Loss: 16099074.0000\n",
      "Epoch 252/100000 | Train Loss: 409320.8125 | Val Loss: 16090488.0000\n",
      "Epoch 253/100000 | Train Loss: 409116.6250 | Val Loss: 16102146.0000\n",
      "Epoch 254/100000 | Train Loss: 408364.3438 | Val Loss: 16105244.0000\n",
      "Epoch 255/100000 | Train Loss: 408502.2500 | Val Loss: 16090657.0000\n",
      "Epoch 256/100000 | Train Loss: 407154.8125 | Val Loss: 16090916.0000\n",
      "Epoch 257/100000 | Train Loss: 407722.6250 | Val Loss: 16102326.0000\n",
      "Epoch 258/100000 | Train Loss: 405769.6562 | Val Loss: 16104944.0000\n",
      "Epoch 259/100000 | Train Loss: 406742.3125 | Val Loss: 16103723.0000\n",
      "Epoch 260/100000 | Train Loss: 404511.4375 | Val Loss: 16094264.0000\n",
      "Epoch 261/100000 | Train Loss: 404953.4375 | Val Loss: 16099487.0000\n",
      "Epoch 262/100000 | Train Loss: 403763.6250 | Val Loss: 16101964.0000\n",
      "Epoch 263/100000 | Train Loss: 402897.0312 | Val Loss: 16103868.0000\n",
      "Epoch 264/100000 | Train Loss: 403374.6875 | Val Loss: 16084024.0000\n",
      "Epoch 265/100000 | Train Loss: 401829.8125 | Val Loss: 16082784.0000\n",
      "Epoch 266/100000 | Train Loss: 402266.3750 | Val Loss: 16077752.0000\n",
      "Epoch 267/100000 | Train Loss: 401204.5000 | Val Loss: 16091371.0000\n",
      "Epoch 268/100000 | Train Loss: 401014.3125 | Val Loss: 16088197.0000\n",
      "Epoch 269/100000 | Train Loss: 400130.5312 | Val Loss: 16056802.0000\n",
      "Epoch 270/100000 | Train Loss: 399858.4688 | Val Loss: 16049268.0000\n",
      "Epoch 271/100000 | Train Loss: 400193.6250 | Val Loss: 16065800.0000\n",
      "Epoch 272/100000 | Train Loss: 398358.0000 | Val Loss: 16073732.0000\n",
      "Epoch 273/100000 | Train Loss: 400653.1250 | Val Loss: 16041130.0000\n",
      "Epoch 274/100000 | Train Loss: 397191.7812 | Val Loss: 16031812.0000\n",
      "Epoch 275/100000 | Train Loss: 400355.5000 | Val Loss: 16045550.0000\n",
      "Epoch 276/100000 | Train Loss: 396053.8750 | Val Loss: 16054200.0000\n",
      "Epoch 277/100000 | Train Loss: 397401.0625 | Val Loss: 16046172.0000\n",
      "Epoch 278/100000 | Train Loss: 394592.4375 | Val Loss: 16029207.0000\n",
      "Epoch 279/100000 | Train Loss: 396602.0000 | Val Loss: 16039254.0000\n",
      "Epoch 280/100000 | Train Loss: 393569.0000 | Val Loss: 16065608.0000\n",
      "Epoch 281/100000 | Train Loss: 395270.9375 | Val Loss: 16046642.0000\n",
      "Epoch 282/100000 | Train Loss: 393252.7500 | Val Loss: 16024006.0000\n",
      "Epoch 283/100000 | Train Loss: 393468.6250 | Val Loss: 16031477.0000\n",
      "Epoch 284/100000 | Train Loss: 391981.2812 | Val Loss: 16044568.0000\n",
      "Epoch 285/100000 | Train Loss: 391693.1250 | Val Loss: 16030732.0000\n",
      "Epoch 286/100000 | Train Loss: 390513.4375 | Val Loss: 16026178.0000\n",
      "Epoch 287/100000 | Train Loss: 389628.0312 | Val Loss: 16015544.0000\n",
      "Epoch 288/100000 | Train Loss: 389455.1562 | Val Loss: 16020422.0000\n",
      "Epoch 289/100000 | Train Loss: 388571.6250 | Val Loss: 16020036.0000\n",
      "Epoch 290/100000 | Train Loss: 388172.6875 | Val Loss: 16013754.0000\n",
      "Epoch 291/100000 | Train Loss: 387450.1562 | Val Loss: 16016462.0000\n",
      "Epoch 292/100000 | Train Loss: 386845.2812 | Val Loss: 16033594.0000\n",
      "Epoch 293/100000 | Train Loss: 386398.3125 | Val Loss: 16033768.0000\n",
      "Epoch 294/100000 | Train Loss: 385911.7500 | Val Loss: 16026940.0000\n",
      "Epoch 295/100000 | Train Loss: 385434.6875 | Val Loss: 16025744.0000\n",
      "Epoch 296/100000 | Train Loss: 385123.1250 | Val Loss: 16036505.0000\n",
      "Epoch 297/100000 | Train Loss: 384386.0000 | Val Loss: 16032257.0000\n",
      "Epoch 298/100000 | Train Loss: 383822.3125 | Val Loss: 16033124.0000\n",
      "Epoch 299/100000 | Train Loss: 383329.9375 | Val Loss: 16030302.0000\n",
      "Epoch 300/100000 | Train Loss: 383091.5625 | Val Loss: 16032157.0000\n",
      "Epoch 301/100000 | Train Loss: 382453.7500 | Val Loss: 16042650.0000\n",
      "Epoch 302/100000 | Train Loss: 381986.5938 | Val Loss: 16039448.0000\n",
      "Epoch 303/100000 | Train Loss: 381834.7500 | Val Loss: 16025260.0000\n",
      "Epoch 304/100000 | Train Loss: 381030.2188 | Val Loss: 16018620.0000\n",
      "Epoch 305/100000 | Train Loss: 382167.4375 | Val Loss: 16034089.0000\n",
      "Epoch 306/100000 | Train Loss: 379898.9375 | Val Loss: 16035401.0000\n",
      "Epoch 307/100000 | Train Loss: 380495.3750 | Val Loss: 16022911.0000\n",
      "Epoch 308/100000 | Train Loss: 379086.3125 | Val Loss: 16016236.0000\n",
      "Epoch 309/100000 | Train Loss: 380216.7500 | Val Loss: 16031073.0000\n",
      "Epoch 310/100000 | Train Loss: 378045.3750 | Val Loss: 16038628.0000\n",
      "Epoch 311/100000 | Train Loss: 378031.4062 | Val Loss: 16032945.0000\n",
      "Epoch 312/100000 | Train Loss: 377063.5625 | Val Loss: 16010384.0000\n",
      "Epoch 313/100000 | Train Loss: 378839.5312 | Val Loss: 16029564.0000\n",
      "Epoch 314/100000 | Train Loss: 376316.6875 | Val Loss: 16031510.0000\n",
      "Epoch 315/100000 | Train Loss: 376816.6875 | Val Loss: 16002332.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      2\u001b[0m save_path \u001b[38;5;241m=\u001b[39m get_next_model_filename(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPIRN_nonuni\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PythonPrograms/PIRN/source/Trainers/Trainer.py:164\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, val_loader, num_epochs, early_stop_patience, save_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m         tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m | \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    165\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain Loss: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_loss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m | Val Loss: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval_loss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/tqdm/std.py:722\u001b[0m, in \u001b[0;36mtqdm.write\u001b[0;34m(cls, s, file, end, nolock)\u001b[0m\n\u001b[1;32m    719\u001b[0m fp \u001b[38;5;241m=\u001b[39m file \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mstdout\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_write_mode(file\u001b[38;5;241m=\u001b[39mfile, nolock\u001b[38;5;241m=\u001b[39mnolock):\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# Write the message\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(end)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/ipykernel/iostream.py:662\u001b[0m, in \u001b[0;36mOutStream.write\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/ipykernel/iostream.py:559\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/ipykernel/iostream.py:266\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     f()\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/zmq/sugar/socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    689\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    690\u001b[0m             data,\n\u001b[1;32m    691\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    692\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    693\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    694\u001b[0m         )\n\u001b[1;32m    695\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "save_path = get_next_model_filename(prefix='PIRN_nonuni')\n",
    "trainer.fit(train_loader, val_loader, num_epochs=100000, early_stop_patience=1000, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc098ba8-ff79-4ab5-b10f-55716c29e29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f13836-e78b-4f38-b958-a9672cf857cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
