{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0275341-ad74-4af1-bcd5-b189db22d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Networks import MLP2\n",
    "from Trainers import NavierStokes_2D_Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79406ac-697c-4519-9c87-040dca2feb13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "input_shape = (2,)  # x,y\n",
    "output_shape = (3,)  # u,v,p\n",
    "\n",
    "model = MLP2(input_shape, output_shape, hidden_layers=[64, 64, 64, 64, 64], activation_function=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a263d72-94b1-4372-855c-71528f2e046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "trainer = NavierStokes_2D_Trainer(model, nu, optimizer, lambda_pde=1e-3, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506532c5-a927-4bcd-b6f6-203d8a6ff156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网格范围\n",
    "x = torch.linspace(0, 2, 64)  # x 范围从 0 到 2，共 64 个点\n",
    "y = torch.linspace(-0.1, 0.1, 16)  # y 范围从 -0.1 到 0.1，共 16 个点\n",
    "\n",
    "# 使用 meshgrid 创建 2D 网格\n",
    "x_grid, y_grid = torch.meshgrid(x, y, indexing='ij')  # 'ij' 保证顺序为 (x, y)\n",
    "\n",
    "# 将网格展平为一个二维张量 (N, 2)，每一行代表一个 (x, y) 坐标\n",
    "collocation_points = torch.stack([x_grid.flatten(), y_grid.flatten()], dim=1)\n",
    "\n",
    "# 生成边界点: y=0.1 和 y=-0.1 时的 (x, y) 点\n",
    "boundary_points_upper = torch.stack([x, torch.full_like(x, 0.1)], dim=1)  # y = 0.1\n",
    "boundary_points_lower = torch.stack([x, torch.full_like(x, -0.1)], dim=1)  # y = -0.1\n",
    "\n",
    "# 生成入口点：x = 0 时，y = linspace(-0.1, 0.1, 16) 对应的 (x, y) 点\n",
    "boundary_points_inlet = torch.stack([torch.full_like(y, 0.0), y], dim=1)  # x = 0, y 范围\n",
    "\n",
    "# 合并为边界点\n",
    "boundary_points = torch.cat([boundary_points_upper, boundary_points_lower, boundary_points_inlet], dim=0)\n",
    "# 边界条件，u, v 在 y=±0.1 处都为 0；在 x=0 时，u=5.5, v=0, p=None\n",
    "boundary_values_u = torch.zeros_like(boundary_points[:, 0])  # u = 0\n",
    "boundary_values_v = torch.zeros_like(boundary_points[:, 0])  # v = 0\n",
    "boundary_values_u[-16:] = 5.5\n",
    "boundary_values_u = boundary_values_u.unsqueeze(1)  # 变为 (batch_size, 1)\n",
    "boundary_values_v = boundary_values_v.unsqueeze(1)\n",
    "boundary_values_p = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe3c8b06-b00c-4326-980f-ce4d12508ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.1000],\n",
       "        [ 0.0000, -0.0867],\n",
       "        [ 0.0000, -0.0733],\n",
       "        ...,\n",
       "        [ 2.0000,  0.0733],\n",
       "        [ 2.0000,  0.0867],\n",
       "        [ 2.0000,  0.1000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebd6a50-3553-4526-be8a-14c633e22722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ffc8fb1ea940d9802bdc22c5c73da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Total Loss: 3.4618e+00 | PDE Loss: 1.3129e-03 | BC Loss: 3.4618e+00\n",
      "Epoch 101/10000, Total Loss: 7.0124e-01 | PDE Loss: 1.3765e+02 | BC Loss: 5.6359e-01\n",
      "Epoch 201/10000, Total Loss: 6.4071e-01 | PDE Loss: 1.6501e+02 | BC Loss: 4.7570e-01\n",
      "Epoch 301/10000, Total Loss: 3.8676e-01 | PDE Loss: 1.0328e+01 | BC Loss: 3.7644e-01\n",
      "Epoch 401/10000, Total Loss: 3.7351e-01 | PDE Loss: 1.9008e-01 | BC Loss: 3.7332e-01\n",
      "Epoch 501/10000, Total Loss: 3.7329e-01 | PDE Loss: 5.7060e-02 | BC Loss: 3.7323e-01\n",
      "Epoch 601/10000, Total Loss: 3.7320e-01 | PDE Loss: 4.0785e-02 | BC Loss: 3.7316e-01\n",
      "Epoch 701/10000, Total Loss: 3.7309e-01 | PDE Loss: 3.3544e-02 | BC Loss: 3.7306e-01\n",
      "Epoch 801/10000, Total Loss: 3.7292e-01 | PDE Loss: 2.8095e-02 | BC Loss: 3.7289e-01\n",
      "Epoch 901/10000, Total Loss: 3.7258e-01 | PDE Loss: 2.4031e-02 | BC Loss: 3.7255e-01\n",
      "Epoch 1001/10000, Total Loss: 3.7166e-01 | PDE Loss: 6.1475e-02 | BC Loss: 3.7160e-01\n",
      "Epoch 1101/10000, Total Loss: 3.6639e-01 | PDE Loss: 3.6055e-01 | BC Loss: 3.6603e-01\n",
      "Epoch 1201/10000, Total Loss: 2.9792e-01 | PDE Loss: 1.6629e+01 | BC Loss: 2.8130e-01\n",
      "Epoch 1301/10000, Total Loss: 2.7555e-01 | PDE Loss: 6.3138e+00 | BC Loss: 2.6924e-01\n",
      "Epoch 1401/10000, Total Loss: 2.6644e-01 | PDE Loss: 4.0245e+00 | BC Loss: 2.6241e-01\n",
      "Epoch 1501/10000, Total Loss: 2.5232e-01 | PDE Loss: 2.7595e+00 | BC Loss: 2.4956e-01\n",
      "Epoch 1601/10000, Total Loss: 2.3353e-01 | PDE Loss: 3.7862e+00 | BC Loss: 2.2974e-01\n",
      "Epoch 1701/10000, Total Loss: 2.2287e-01 | PDE Loss: 2.5849e+00 | BC Loss: 2.2029e-01\n",
      "Epoch 1801/10000, Total Loss: 5.1657e-01 | PDE Loss: 1.4714e+02 | BC Loss: 3.6942e-01\n",
      "Epoch 1901/10000, Total Loss: 4.2154e-01 | PDE Loss: 9.7686e+01 | BC Loss: 3.2385e-01\n",
      "Epoch 2001/10000, Total Loss: 3.5938e-01 | PDE Loss: 8.6276e+01 | BC Loss: 2.7310e-01\n",
      "Epoch 2101/10000, Total Loss: 3.3970e-01 | PDE Loss: 8.1156e+01 | BC Loss: 2.5855e-01\n",
      "Epoch 2201/10000, Total Loss: 3.2763e-01 | PDE Loss: 7.6701e+01 | BC Loss: 2.5093e-01\n",
      "Epoch 2301/10000, Total Loss: 3.4502e-01 | PDE Loss: 7.7359e+01 | BC Loss: 2.6766e-01\n",
      "Epoch 2401/10000, Total Loss: 3.2095e-01 | PDE Loss: 6.6811e+01 | BC Loss: 2.5414e-01\n",
      "Epoch 2501/10000, Total Loss: 3.0602e-01 | PDE Loss: 6.4571e+01 | BC Loss: 2.4145e-01\n",
      "Epoch 2601/10000, Total Loss: 3.0390e-01 | PDE Loss: 6.0917e+01 | BC Loss: 2.4298e-01\n",
      "Epoch 2701/10000, Total Loss: 2.9962e-01 | PDE Loss: 5.9849e+01 | BC Loss: 2.3977e-01\n",
      "Epoch 2801/10000, Total Loss: 2.9733e-01 | PDE Loss: 5.7792e+01 | BC Loss: 2.3953e-01\n",
      "Epoch 2901/10000, Total Loss: 2.9371e-01 | PDE Loss: 5.7405e+01 | BC Loss: 2.3631e-01\n",
      "Epoch 3001/10000, Total Loss: 2.9046e-01 | PDE Loss: 5.4679e+01 | BC Loss: 2.3578e-01\n",
      "Epoch 3101/10000, Total Loss: 2.8333e-01 | PDE Loss: 5.3387e+01 | BC Loss: 2.2995e-01\n",
      "Epoch 3201/10000, Total Loss: 2.7168e-01 | PDE Loss: 4.4821e+01 | BC Loss: 2.2686e-01\n",
      "Epoch 3301/10000, Total Loss: 2.6077e-01 | PDE Loss: 3.8216e+01 | BC Loss: 2.2256e-01\n",
      "Epoch 3401/10000, Total Loss: 2.5413e-01 | PDE Loss: 3.2136e+01 | BC Loss: 2.2200e-01\n",
      "Epoch 3501/10000, Total Loss: 2.4649e-01 | PDE Loss: 2.8162e+01 | BC Loss: 2.1833e-01\n",
      "Epoch 3601/10000, Total Loss: 2.4381e-01 | PDE Loss: 2.4681e+01 | BC Loss: 2.1913e-01\n",
      "Epoch 3701/10000, Total Loss: 2.3784e-01 | PDE Loss: 2.1910e+01 | BC Loss: 2.1593e-01\n",
      "Epoch 3801/10000, Total Loss: 2.3509e-01 | PDE Loss: 1.9988e+01 | BC Loss: 2.1510e-01\n",
      "Epoch 3901/10000, Total Loss: 2.3205e-01 | PDE Loss: 1.7552e+01 | BC Loss: 2.1450e-01\n",
      "Epoch 4001/10000, Total Loss: 2.2982e-01 | PDE Loss: 1.5974e+01 | BC Loss: 2.1384e-01\n",
      "Epoch 4101/10000, Total Loss: 6.7797e-01 | PDE Loss: 1.5640e+02 | BC Loss: 5.2157e-01\n",
      "Epoch 4201/10000, Total Loss: 5.2691e-01 | PDE Loss: 1.3699e+02 | BC Loss: 3.8991e-01\n",
      "Epoch 4301/10000, Total Loss: 4.6883e-01 | PDE Loss: 1.1823e+02 | BC Loss: 3.5060e-01\n",
      "Epoch 4401/10000, Total Loss: 4.0137e-01 | PDE Loss: 8.7177e+01 | BC Loss: 3.1419e-01\n",
      "Epoch 4501/10000, Total Loss: 3.7148e-01 | PDE Loss: 8.3297e+01 | BC Loss: 2.8819e-01\n",
      "Epoch 4601/10000, Total Loss: 3.5272e-01 | PDE Loss: 8.0542e+01 | BC Loss: 2.7217e-01\n",
      "Epoch 4701/10000, Total Loss: 3.3948e-01 | PDE Loss: 7.8419e+01 | BC Loss: 2.6107e-01\n",
      "Epoch 4801/10000, Total Loss: 3.3058e-01 | PDE Loss: 7.6305e+01 | BC Loss: 2.5427e-01\n",
      "Epoch 4901/10000, Total Loss: 3.2672e-01 | PDE Loss: 7.3544e+01 | BC Loss: 2.5317e-01\n",
      "Epoch 5001/10000, Total Loss: 3.3750e-01 | PDE Loss: 6.6760e+01 | BC Loss: 2.7074e-01\n",
      "Epoch 5101/10000, Total Loss: 3.1436e-01 | PDE Loss: 7.0473e+01 | BC Loss: 2.4389e-01\n",
      "Epoch 5201/10000, Total Loss: 3.1158e-01 | PDE Loss: 6.7011e+01 | BC Loss: 2.4456e-01\n",
      "Epoch 5301/10000, Total Loss: 3.0708e-01 | PDE Loss: 6.7515e+01 | BC Loss: 2.3957e-01\n",
      "Epoch 5401/10000, Total Loss: 3.0307e-01 | PDE Loss: 6.3532e+01 | BC Loss: 2.3954e-01\n",
      "Epoch 5501/10000, Total Loss: 3.0275e-01 | PDE Loss: 6.5746e+01 | BC Loss: 2.3700e-01\n",
      "Epoch 5601/10000, Total Loss: 2.9448e-01 | PDE Loss: 6.1125e+01 | BC Loss: 2.3335e-01\n",
      "Epoch 5701/10000, Total Loss: 2.9429e-01 | PDE Loss: 6.1746e+01 | BC Loss: 2.3255e-01\n",
      "Epoch 5801/10000, Total Loss: 2.7951e-01 | PDE Loss: 5.2462e+01 | BC Loss: 2.2705e-01\n",
      "Epoch 5901/10000, Total Loss: 2.9109e-01 | PDE Loss: 6.2743e+01 | BC Loss: 2.2835e-01\n",
      "Epoch 6001/10000, Total Loss: 3.2159e-01 | PDE Loss: 5.9037e+01 | BC Loss: 2.6255e-01\n",
      "Epoch 6101/10000, Total Loss: 3.0379e-01 | PDE Loss: 6.1943e+01 | BC Loss: 2.4184e-01\n",
      "Epoch 6201/10000, Total Loss: 2.9720e-01 | PDE Loss: 5.9551e+01 | BC Loss: 2.3765e-01\n",
      "Epoch 6301/10000, Total Loss: 2.9393e-01 | PDE Loss: 5.4078e+01 | BC Loss: 2.3985e-01\n",
      "Epoch 6401/10000, Total Loss: 2.7909e-01 | PDE Loss: 5.0447e+01 | BC Loss: 2.2864e-01\n",
      "Epoch 6501/10000, Total Loss: 2.6034e-01 | PDE Loss: 3.9506e+01 | BC Loss: 2.2083e-01\n",
      "Epoch 6601/10000, Total Loss: 2.4754e-01 | PDE Loss: 3.0102e+01 | BC Loss: 2.1744e-01\n",
      "Epoch 6701/10000, Total Loss: 5.8434e-01 | PDE Loss: 1.1992e+02 | BC Loss: 4.6442e-01\n",
      "Epoch 6801/10000, Total Loss: 3.8117e-01 | PDE Loss: 6.1322e+01 | BC Loss: 3.1985e-01\n",
      "Epoch 6901/10000, Total Loss: 3.4032e-01 | PDE Loss: 5.3173e+01 | BC Loss: 2.8714e-01\n",
      "Epoch 7001/10000, Total Loss: 3.2447e-01 | PDE Loss: 5.2062e+01 | BC Loss: 2.7240e-01\n",
      "Epoch 7101/10000, Total Loss: 3.3233e-01 | PDE Loss: 5.4997e+01 | BC Loss: 2.7733e-01\n",
      "Epoch 7201/10000, Total Loss: 3.0929e-01 | PDE Loss: 5.2676e+01 | BC Loss: 2.5661e-01\n",
      "Epoch 7301/10000, Total Loss: 3.0510e-01 | PDE Loss: 5.2740e+01 | BC Loss: 2.5236e-01\n",
      "Epoch 7401/10000, Total Loss: 3.0631e-01 | PDE Loss: 5.0355e+01 | BC Loss: 2.5596e-01\n",
      "Epoch 7501/10000, Total Loss: 3.0010e-01 | PDE Loss: 5.3703e+01 | BC Loss: 2.4640e-01\n",
      "Epoch 7601/10000, Total Loss: 3.0174e-01 | PDE Loss: 5.1283e+01 | BC Loss: 2.5046e-01\n",
      "Epoch 7701/10000, Total Loss: 2.9605e-01 | PDE Loss: 5.4056e+01 | BC Loss: 2.4199e-01\n",
      "Epoch 7801/10000, Total Loss: 2.9607e-01 | PDE Loss: 5.2967e+01 | BC Loss: 2.4310e-01\n",
      "Epoch 7901/10000, Total Loss: 2.9430e-01 | PDE Loss: 5.4648e+01 | BC Loss: 2.3965e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollocation_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_values_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_values_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_values_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/PINN.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PythonPrograms/PIRN/source/Trainers/Trainer.py:441\u001b[0m, in \u001b[0;36mNavierStokes_2D_Trainer.train\u001b[0;34m(self, collocation_points, boundary_points, boundary_values_u, boundary_values_v, boundary_values_p, epochs, save_path)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpde_loss\u001b[38;5;241m.\u001b[39mappend(pde_loss)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# 计算边界条件损失\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m u_b, v_b, p_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboundary_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 将 boundary_points 传递给模型\u001b[39;00m\n\u001b[1;32m    442\u001b[0m bc_loss_u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(u_b, boundary_values_u)\n\u001b[1;32m    443\u001b[0m bc_loss_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(v_b, boundary_values_v)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PythonPrograms/PIRN/source/Networks/MLP.py:92\u001b[0m, in \u001b[0;36mMLP2.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     90\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 合并 x 和 y 为 (batch_size, 2)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 通过网络进行前向传播\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 分别计算 u, v 和 p\u001b[39;00m\n\u001b[1;32m     95\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_u(hidden)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/activation.py:366\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train(collocation_points, boundary_points, boundary_values_u, boundary_values_v, boundary_values_p, epochs=10000, save_path='model/PINN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ddc1b47-0320-4f56-a10d-87eb401844dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3499a7858184cb196e1cc0cb2673531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whw/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([144])) that is different to the input size (torch.Size([1024, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Total Loss: 2.9877e+00 | PDE Loss: 3.8941e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 101/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.1274e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 201/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.5600e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 301/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.1342e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 401/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.8509e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 501/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.6760e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 601/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.2954e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 701/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.9850e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 801/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.3027e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 901/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.5047e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1001/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.6409e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1101/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.7436e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1201/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.8306e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1301/10000, Total Loss: 2.9877e+00 | PDE Loss: 7.9167e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1401/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.0075e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1501/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.1023e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1601/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.2039e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1701/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.3091e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1801/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.4207e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 1901/10000, Total Loss: 2.9877e+00 | PDE Loss: 8.5366e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 2001/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.7809e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2101/10000, Total Loss: 2.9877e+00 | PDE Loss: 9.7710e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 2201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0011e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0235e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0430e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2501/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0611e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2601/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0792e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2701/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0976e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2801/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1162e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 2901/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0814e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3001/10000, Total Loss: 2.9877e+00 | PDE Loss: 9.8749e-08 | BC Loss: 2.9877e+00\n",
      "Epoch 3101/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0127e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0320e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0483e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0624e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3501/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0759e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3601/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.0884e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3701/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1008e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3801/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1134e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 3901/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1252e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4001/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1368e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4101/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1481e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1589e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1692e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.1792e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4501/10000, Total Loss: 2.9877e+00 | PDE Loss: 2.6723e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4601/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.8285e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4701/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7648e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4801/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7373e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 4901/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7281e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5001/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7283e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5101/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7339e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7422e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7521e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7630e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5501/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.7429e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5601/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3858e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5701/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3808e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5801/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3838e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 5901/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3886e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6001/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3937e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6101/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.3984e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4032e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4080e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4130e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6501/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4179e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6601/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4229e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6701/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4281e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6801/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4333e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 6901/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4386e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7001/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4441e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7101/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4495e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7201/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4550e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7301/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4604e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7401/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4658e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7501/10000, Total Loss: 2.9877e+00 | PDE Loss: 1.4709e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7601/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.9889e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7701/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.9188e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7801/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.9596e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 7901/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.9437e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8001/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.8910e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8101/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.8166e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8201/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.7289e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8301/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.6359e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8401/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.5406e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8501/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.4448e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8601/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.3488e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8701/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.2533e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8801/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.1591e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 8901/10000, Total Loss: 2.9877e+00 | PDE Loss: 5.0654e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9001/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.9729e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9101/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.8821e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9201/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.7927e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9301/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.7050e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9401/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.6189e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9501/10000, Total Loss: 2.9877e+00 | PDE Loss: 4.5345e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9601/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.3736e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9701/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.2438e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9801/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.1514e-07 | BC Loss: 2.9877e+00\n",
      "Epoch 9901/10000, Total Loss: 2.9877e+00 | PDE Loss: 6.0651e-07 | BC Loss: 2.9877e+00\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "trainer = NavierStokes_2D_Trainer(model, nu, optimizer, lambda_pde=1e-7, device='cuda')\n",
    "trainer.train(collocation_points, boundary_points, boundary_values_u, boundary_values_v, boundary_values_p, epochs=10000, save_path='model/PINN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5e4ec-2a0c-4ef0-830b-8ae3b315a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
